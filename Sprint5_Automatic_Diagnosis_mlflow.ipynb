{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint5:  Automatic_Diagnosis_mlflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKlUZxrWVBzSY9dyXiBWD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-torki/ECG-Classification/blob/main/Sprint5_Automatic_Diagnosis_mlflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcuRK9ud-IdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1997171a-b9f1-43ae-9756-f7eca328fb33"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_89Ej9E-Mz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "480acaaa-1103-41ae-fb74-ad6388c33d26"
      },
      "source": [
        "cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZBLjEbVMyS7",
        "outputId": "5e9f39f5-e4f0-4346-c260-13b49b58e943"
      },
      "source": [
        "!pip install mlflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-1.19.0-py3-none-any.whl (14.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.4 MB 61 kB/s \n",
            "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.15.0.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.19.5)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.20)\n",
            "Collecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 45.0 MB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
            "Collecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic<=1.4.1->mlflow) (2.8.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.1.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (4.6.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow) (57.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->mlflow) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (2.4.7)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n",
            "Building wheels for collected packages: alembic, databricks-cli, prometheus-flask-exporter\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158170 sha256=2d12eb3083d53c6cd00b93010bf3056b5c7e9b87e239120ce1d3fd7dd34be4e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.15.0-py3-none-any.whl size=105259 sha256=1012ae0ae8b384cdd1dd0714a46b40f36c2b1e7417e9ff813027d24d4545f79f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/ba/75/284f9a90ff7a010bb23b9798f2e9a19dd9fe619379c917bff4\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17416 sha256=e2a15420ae625ca5366decf0254abfd54433044aba7229621bb9b4b415cb5fb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/1e/1c/c765920cb92b2f0343d2dd8b481a407cee2823f9b4bbd2e52a\n",
            "Successfully built alembic databricks-cli prometheus-flask-exporter\n",
            "Installing collected packages: smmap, websocket-client, python-editor, Mako, gitdb, querystring-parser, pyyaml, prometheus-flask-exporter, gunicorn, gitpython, docker, databricks-cli, alembic, mlflow\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.1.4 alembic-1.4.1 databricks-cli-0.15.0 docker-5.0.0 gitdb-4.0.7 gitpython-3.1.18 gunicorn-20.1.0 mlflow-1.19.0 prometheus-flask-exporter-0.18.2 python-editor-1.0.4 pyyaml-5.4.1 querystring-parser-1.2.4 smmap-4.0.0 websocket-client-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_JvBxhdQxn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65c5738-1ac9-41cc-ca9c-1ff21e2af1a9"
      },
      "source": [
        "!pip install wfdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-3.4.0-py3-none-any.whl (137 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 102 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 112 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 122 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 133 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 137 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.10.0)\n",
            "Requirement already satisfied: idna>=2.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2016.8.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2021.5.30)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.22.2.post1)\n",
            "Collecting threadpoolctl>=1.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.4.7)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.19.5)\n",
            "Requirement already satisfied: urllib3>=1.22 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.24.3)\n",
            "Requirement already satisfied: chardet>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.0.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10.0->wfdb) (1.15.0)\n",
            "Installing collected packages: threadpoolctl, wfdb\n",
            "Successfully installed threadpoolctl-2.2.0 wfdb-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uj4z8kdQ2Wj"
      },
      "source": [
        "from wfdb import plot_items\n",
        "from wfdb.processing import resample_sig , find_peaks , resample_singlechan\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "import mlflow.tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Izi3-gbz9h"
      },
      "source": [
        "#Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izRhn2Wz5yBA"
      },
      "source": [
        "#utils:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p36fjh4IoCWv",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import scipy\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import pywt\n",
        "from wfdb.processing import resample_sig\n",
        "from scipy.signal import kaiserord, firwin, filtfilt, butter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z_r1HYb2KFo",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def madev(d, axis=None):\n",
        "    \"\"\" Mean absolute deviation of a signal \"\"\"\n",
        "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
        "\n",
        "def wavelet_denoising(x, wavelet='db4', level=1):\n",
        "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
        "    # print(madev(coeff[-level]))\n",
        "    sigma = (1/0.6745) * madev(coeff[-level])\n",
        "    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n",
        "    # print('sigma' , sigma ,'uthresh' , uthresh)\n",
        "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='soft') for i in coeff[1:])\n",
        "    return pywt.waverec(coeff, wavelet, mode='per')\n",
        "\n",
        "\n",
        "def FIRRemoveBL(ecgy, Fs, Fc, factor):\n",
        "    \n",
        "    #    ecgy:        the contamined signal (must be a list)\n",
        "    #    Fc:          cut-off frequency\n",
        "    #    Fs:          sample frequiency\n",
        "    #    ECG_Clean :  processed signal without BLW\n",
        "    \n",
        "    # getting the length of the signal\n",
        "    signal_len = len(ecgy)\n",
        "    \n",
        "    # The Nyquist rate of the signal.\n",
        "    nyq_rate = Fs / 2.0\n",
        "    \n",
        "    # The desired width of the transition from stop to pass,\n",
        "    # relative to the Nyquist rate. \n",
        "    width = 0.07/nyq_rate \n",
        "    \n",
        "    # Attenuation in the stop band, in dB.\n",
        "    # related to devs in Matlab. On Matlab is on proportion\n",
        "    ripple_db = round(-20*np.log10(0.001))+1\n",
        "    ripple_db = ripple_db / factor\n",
        "\n",
        "    \n",
        "    # Compute the order and Kaiser parameter for the FIR filter.\n",
        "    N, beta = kaiserord(ripple_db, width)\n",
        "    N |= 1\n",
        "    # Use firwin with a Kaiser window to create a highpass FIR filter.\n",
        "    h = firwin(N, Fc/nyq_rate, window=('kaiser', beta), pass_zero='highpass')\n",
        "\n",
        "    # Check filtfilt condition\n",
        "    if N*3 > signal_len:\n",
        "        diff = N*3 - signal_len\n",
        "        ecgy = list(reversed(ecgy)) + list(ecgy) + list(ecgy[-1] * np.ones(diff))\n",
        "        \n",
        "        # Filtering with filtfilt\n",
        "        ECG_Clean = filtfilt(h, 1.0, ecgy)\n",
        "        ECG_Clean = ECG_Clean[signal_len: signal_len + signal_len]\n",
        "    else:\n",
        "        ECG_Clean = filtfilt(h, 1.0, ecgy)\n",
        "    \n",
        "    return ECG_Clean, N\n",
        "\n",
        "\n",
        "def FIRRemoveHF(ecgy, Fs, Fc, factor):\n",
        "    #    ecgy:        the contamined signal (must be a list)\n",
        "    #    Fc:          cut-off frequency\n",
        "    #    Fs:          sample frequiency\n",
        "    #    ECG_Clean :  processed signal without BLW\n",
        "\n",
        "    # getting the length of the signal\n",
        "    signal_len = len(ecgy)\n",
        "\n",
        "    # The Nyquist rate of the signal.\n",
        "    nyq_rate = Fs / 2.0\n",
        "\n",
        "    # The desired width of the transition from stop to pass,\n",
        "    # relative to the Nyquist rate.\n",
        "    width = 0.07 / nyq_rate\n",
        "\n",
        "    # Attenuation in the stop band, in dB.\n",
        "    # related to devs in Matlab. On Matlab is on proportion\n",
        "    ripple_db = round(-20 * np.log10(0.001)) + 1\n",
        "    ripple_db = ripple_db / factor\n",
        "\n",
        "    # Compute the order and Kaiser parameter for the FIR filter.\n",
        "    N, beta = kaiserord(ripple_db, width)\n",
        "    N |= 1\n",
        "    # Use firwin with a Kaiser window to create a highpass FIR filter.\n",
        "    h = firwin(N, Fc / nyq_rate, window=('kaiser', beta), pass_zero='lowpass')\n",
        "\n",
        "    # Check filtfilt condition\n",
        "    if N * 3 > signal_len:\n",
        "        diff = N * 3 - signal_len\n",
        "        ecgy = list(reversed(ecgy)) + list(ecgy) + list(ecgy[-1] * np.ones(diff))\n",
        "\n",
        "        # Filtering with filtfilt\n",
        "        ECG_Clean = filtfilt(h, 1.0, ecgy)\n",
        "        ECG_Clean = ECG_Clean[signal_len: signal_len + signal_len]\n",
        "    else:\n",
        "        ECG_Clean = filtfilt(h, 1.0, ecgy)\n",
        "\n",
        "    return ECG_Clean, N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nOBehFtmo8Q",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dd6e5705-e40f-4c77-c607-2f7c5dbed14b"
      },
      "source": [
        "#@title\n",
        "#################### Constants and Arguments  ##########################\n",
        "\n",
        "path_to_data_dir = './export/raw/'\n",
        "\n",
        "leads = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'v3', 'v4', 'v5', 'v6'] \n",
        "n_channel = 12\n",
        "fs_ini = 350\n",
        "fs_target = 400\n",
        "n_sample = 4096\n",
        "down_sample = 1\n",
        "filename = sorted(glob(str(path_to_data_dir)+'*.txt'))\n",
        "filename[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./export/raw/1.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itUcdWhjqLgm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdbzAdrn9M_J",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# from utils import *\n",
        "\n",
        "class DataProvider(object):\n",
        "    def __init__(self,data_files,n_class,\n",
        "                 n_sample=4096,\n",
        "                 n_channel=12, \n",
        "                 fs_ini = 350,\n",
        "                 fs_target = None,\n",
        "                 wav = None,\n",
        "                 wav_level = None,\n",
        "                 Fc_l = None,\n",
        "                 down_sample = 1,\n",
        "                 filtering =1 ):\n",
        "\n",
        "        self.data_files = data_files\n",
        "        self.n_channel = n_channel\n",
        "        self.fs_ini = fs_ini\n",
        "        self.n_sample = n_sample\n",
        "        self.down_sample = down_sample\n",
        "        self.filtering = filtering\n",
        "\n",
        "        self.fs_target = fs_target\n",
        "\n",
        "        self.wav = wav\n",
        "        self.level = wav_level\n",
        "        self.Fc_l = Fc_l\n",
        "            # self.Fc_h = Fc_h \n",
        "\n",
        "\n",
        "    def pre_process(self , signal):\n",
        "\n",
        "        if self.down_sample:\n",
        "            resampled = []\n",
        "            for j in range(self.n_channel):\n",
        "                # print(signal.shape)\n",
        "                s = resample_sig(signal[j,:] , fs=self.fs_ini , fs_target=self.fs_target)[0][:self.n_sample]\n",
        "                # print(s.shape)\n",
        "                if s.shape[0] < self.n_sample:\n",
        "                    a = (self.n_sample - s.shape[0])//2\n",
        "                    s = np.pad( s,(a,a))\n",
        "                resampled.append(s)\n",
        "        else:\n",
        "            resampled = signal\n",
        "            self.fs_target = self.fs_ini\n",
        "        resampled = np.array(resampled)\n",
        "\n",
        "        if self.filtering:\n",
        "            fil = []\n",
        "            for j in range(self.n_channel):\n",
        "                # print(resampled.shape)\n",
        "                sa = resampled[j,:]\n",
        "                # print(sa.shape)\n",
        "                sa_f , N = FIRRemoveBL(sa , self.fs_target, self.Fc_l , factor=4.5)\n",
        "                # sa_f, N = FIRRemoveHF(sa_f, self.fs_target, self.Fc_h, 4.5)\n",
        "                sa_f =  wavelet_denoising(sa_f, wavelet=self.wav, level=self.level)\n",
        "                fil.append(sa_f)\n",
        "        else:\n",
        "            fil = resampled\n",
        "\n",
        "        fil = np.array(fil)\n",
        "        # print(fil.shape)\n",
        "\n",
        "        if fil.shape[-1] != self.n_channel:\n",
        "            fil = np.transpose(fil)\n",
        "            fil = np.array(fil)\n",
        "\n",
        "        return fil\n",
        "\n",
        "\n",
        "    def __call__(self,n): \n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "        for f in self.data_files[:n]: \n",
        "            rec = np.loadtxt(f , skiprows=1 , delimiter=';'  , unpack=True)\n",
        "            # if rec[0,0] > 100:\n",
        "            rec = rec/100\n",
        "            # print(rec.shape)\n",
        "            rec = self.pre_process(rec)\n",
        "            # print(rec.shape)\n",
        "            X.append(rec)\n",
        "        \n",
        "        X = np.array(X)\n",
        "        # Y = np.array(Y)\n",
        "\n",
        "        return X#,Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx24HI1M5OnR",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "dp = DataProvider(data_files=filename, n_class=6 , fs_target=400 ,\n",
        "                  wav='sym5' , wav_level=1 , Fc_l =0.67,\n",
        "                  down_sample=1 , filtering=1)\n",
        "X = dp(5)\n",
        "print(X.shape)\n",
        "\n",
        "t = np.arange(0,10,10/4096)\n",
        "x = np.loadtxt(filename[1] , skiprows=1 , delimiter=';'  , unpack=True)[0,:4096]/100\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.grid(color = 'grey', linestyle = '--', linewidth = 0.5)\n",
        "plt.title('Both Downsampling & Filtering' , fontsize=14)\n",
        "plt.plot(t , x ,'black', label='Raw')\n",
        "plt.plot(t ,X[1,:,0] , 'lightcoral',label='Filtered')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyUIlb8d6xJ_",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "dp = DataProvider(data_files=filename, n_class=6 , fs_target=400 ,\n",
        "                  down_sample=1 , filtering=0)\n",
        "X = dp(5)\n",
        "print(X.shape)\n",
        "\n",
        "t = np.arange(0,10,10/4096)\n",
        "x = np.loadtxt(filename[1] , skiprows=1 , delimiter=';'  , unpack=True)[0,:4096]/100\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.grid(color = 'grey', linestyle = '--', linewidth = 0.5)\n",
        "plt.title('With Downsampling , without filtering' , fontsize=14)\n",
        "plt.plot(t , x ,'black', label='Raw')\n",
        "plt.plot(t ,X[1,:,0] , 'lightcoral',label='Filtered')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmMpI4zw7EPk",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "dp = DataProvider(data_files=filename, n_class=6 , fs_target=400 ,\n",
        "                  down_sample=0 , filtering=0)\n",
        "X = dp(5)\n",
        "print(X.shape)\n",
        "\n",
        "t = np.arange(0,10,10/6000)\n",
        "x = np.loadtxt(filename[1] , skiprows=1 , delimiter=';'  , unpack=True)[0,:6000]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.grid(color = 'grey', linestyle = '--', linewidth = 0.5)\n",
        "plt.title('With niether filtering nor downsampling' , fontsize=14)\n",
        "plt.plot(t , x ,'black', label='Raw')\n",
        "plt.plot(t ,X[1,:,0] , 'lightcoral',label='Filtered')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLy4AO2X7Qts",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "dp = DataProvider(data_files=filename, n_class=6 ,\n",
        "                wav='sym5' , wav_level=1 , Fc_l =0.67,\n",
        "                  down_sample=0 , filtering=1)\n",
        "X = dp(5)\n",
        "print(X.shape)\n",
        "\n",
        "t = np.arange(0,10,10/6000)\n",
        "x = np.loadtxt(filename[1] , skiprows=1 , delimiter=';'  , unpack=True)[0,:6000]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.grid(color = 'grey', linestyle = '--', linewidth = 0.5)\n",
        "plt.title('with filtering , without downsampling' , fontsize=14)\n",
        "plt.plot(t , x ,'black', label='Raw')\n",
        "plt.plot(t ,X[1,:,0] , 'lightcoral',label='Filtered')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh445QuAAqAR",
        "cellView": "form"
      },
      "source": [
        "#@title\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cFD_eKGSVhX"
      },
      "source": [
        "#Model definition, training & predictions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6byOKWWd_xw"
      },
      "source": [
        "Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D1nNH5Ud-Sg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ig75yuxRWEE",
        "outputId": "2b6df6c1-d233-4517-dc95-74ad83799030"
      },
      "source": [
        "cd ./ecg_ptbxl_benchmarking/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/.shortcut-targets-by-id/1j3ncHY23bba4nXCRAt52Fr8YgGZpml9-/ecg_ptbxl_benchmarking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ytFYZfLRnMp",
        "outputId": "c9748944-b195-4666-d818-db88c2da4cd7"
      },
      "source": [
        "cd code/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/.shortcut-targets-by-id/1j3ncHY23bba4nXCRAt52Fr8YgGZpml9-/ecg_ptbxl_benchmarking/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3qSI1C11wkq",
        "cellView": "form"
      },
      "source": [
        "#@title utils\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import glob\n",
        "import pickle\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import wfdb\n",
        "import ast\n",
        "from sklearn.metrics import classification_report, fbeta_score, roc_auc_score, roc_curve, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
        "import warnings\n",
        "\n",
        "# EVALUATION STUFF\n",
        "def generate_results(idxs, y_true, y_pred, thresholds):\n",
        "    return evaluate_experiment(y_true[idxs], y_pred[idxs], thresholds)\n",
        "\n",
        "def evaluate_experiment(y_true, y_pred, thresholds=None):\n",
        "    results = {}\n",
        "\n",
        "    if not thresholds is None:\n",
        "        # binary predictions\n",
        "        y_pred_binary = apply_thresholds(y_pred, thresholds)\n",
        "        # PhysioNet/CinC Challenges metrics\n",
        "        challenge_scores = challenge_metrics(y_true, y_pred_binary, beta1=2, beta2=2)\n",
        "        results['F_beta_macro'] = challenge_scores['F_beta_macro']\n",
        "        results['G_beta_macro'] = challenge_scores['G_beta_macro']\n",
        "\n",
        "    # label based metric\n",
        "    results['macro_auc'] = roc_auc_score(y_true, y_pred, average='macro')\n",
        "    \n",
        "    df_result = pd.DataFrame(results, index=[0])\n",
        "    return df_result\n",
        "\n",
        "def challenge_metrics(y_true, y_pred, beta1=2, beta2=2, class_weights=None, single=False):\n",
        "    f_beta = 0\n",
        "    g_beta = 0\n",
        "    if single: # if evaluating single class in case of threshold-optimization\n",
        "        sample_weights = np.ones(y_true.sum(axis=1).shape)\n",
        "    else:\n",
        "        sample_weights = y_true.sum(axis=1)\n",
        "    for classi in range(y_true.shape[1]):\n",
        "        y_truei, y_predi = y_true[:,classi], y_pred[:,classi]\n",
        "        TP, FP, TN, FN = 0.,0.,0.,0.\n",
        "        for i in range(len(y_predi)):\n",
        "            sample_weight = sample_weights[i]\n",
        "            if y_truei[i]==y_predi[i]==1: \n",
        "                TP += 1./sample_weight\n",
        "            if ((y_predi[i]==1) and (y_truei[i]!=y_predi[i])): \n",
        "                FP += 1./sample_weight\n",
        "            if y_truei[i]==y_predi[i]==0: \n",
        "                TN += 1./sample_weight\n",
        "            if ((y_predi[i]==0) and (y_truei[i]!=y_predi[i])): \n",
        "                FN += 1./sample_weight \n",
        "        f_beta_i = ((1+beta1**2)*TP)/((1+beta1**2)*TP + FP + (beta1**2)*FN)\n",
        "        g_beta_i = (TP)/(TP+FP+beta2*FN)\n",
        "\n",
        "        f_beta += f_beta_i\n",
        "        g_beta += g_beta_i\n",
        "\n",
        "    return {'F_beta_macro':f_beta/y_true.shape[1], 'G_beta_macro':g_beta/y_true.shape[1]}\n",
        "\n",
        "def get_appropriate_bootstrap_samples(y_true, n_bootstraping_samples):\n",
        "    samples=[]\n",
        "    while True:\n",
        "        ridxs = np.random.randint(0, len(y_true), len(y_true))\n",
        "        if y_true[ridxs].sum(axis=0).min() != 0:\n",
        "            samples.append(ridxs)\n",
        "            if len(samples) == n_bootstraping_samples:\n",
        "                break\n",
        "    return samples\n",
        "\n",
        "def find_optimal_cutoff_threshold(target, predicted):\n",
        "    \"\"\" \n",
        "    Find the optimal probability cutoff point for a classification model related to event rate\n",
        "    \"\"\"\n",
        "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "def find_optimal_cutoff_thresholds(y_true, y_pred):\n",
        "\treturn [find_optimal_cutoff_threshold(y_true[:,i], y_pred[:,i]) for i in range(y_true.shape[1])]\n",
        "\n",
        "def find_optimal_cutoff_threshold_for_Gbeta(target, predicted, n_thresholds=100):\n",
        "    thresholds = np.linspace(0.00,1,n_thresholds)\n",
        "    scores = [challenge_metrics(target, predicted>t, single=True)['G_beta_macro'] for t in thresholds]\n",
        "    optimal_idx = np.argmax(scores)\n",
        "    return thresholds[optimal_idx]\n",
        "\n",
        "def find_optimal_cutoff_thresholds_for_Gbeta(y_true, y_pred):\n",
        "    print(\"optimize thresholds with respect to G_beta\")\n",
        "    return [find_optimal_cutoff_threshold_for_Gbeta(y_true[:,k][:,np.newaxis], y_pred[:,k][:,np.newaxis]) for k in tqdm(range(y_true.shape[1]))]\n",
        "\n",
        "def apply_thresholds(preds, thresholds):\n",
        "\t\"\"\"\n",
        "\t\tapply class-wise thresholds to prediction score in order to get binary format.\n",
        "\t\tBUT: if no score is above threshold, pick maximum. This is needed due to metric issues.\n",
        "\t\"\"\"\n",
        "\ttmp = []\n",
        "\tfor p in preds:\n",
        "\t\ttmp_p = (p > thresholds).astype(int)\n",
        "\t\tif np.sum(tmp_p) == 0:\n",
        "\t\t\ttmp_p[np.argmax(p)] = 1\n",
        "\t\ttmp.append(tmp_p)\n",
        "\ttmp = np.array(tmp)\n",
        "\treturn tmp\n",
        "\n",
        "# DATA PROCESSING STUFF\n",
        "\n",
        "def load_dataset(path, sampling_rate, release=False):\n",
        "    if path.split('/')[-2] == 'ptbxl':\n",
        "        # load and convert annotation data\n",
        "        Y = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
        "        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        # Load raw signal data\n",
        "        X = load_raw_data_ptbxl(Y, sampling_rate, path)\n",
        "\n",
        "    elif path.split('/')[-2] == 'ICBEB':\n",
        "        # load and convert annotation data\n",
        "        Y = pd.read_csv(path+'icbeb_database.csv', index_col='ecg_id')\n",
        "        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        # Load raw signal data\n",
        "        X = load_raw_data_icbeb(Y, sampling_rate, path)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def load_raw_data_icbeb(df, sampling_rate, path):\n",
        "\n",
        "    if sampling_rate == 100:\n",
        "        if os.path.exists(path + 'raw100.npy'):\n",
        "            data = np.load(path+'raw100.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path + 'records100/'+str(f)) for f in tqdm(df.index)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw100.npy', 'wb'), protocol=4)\n",
        "    elif sampling_rate == 500:\n",
        "        if os.path.exists(path + 'raw500.npy'):\n",
        "            data = np.load(path+'raw500.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path + 'records500/'+str(f)) for f in tqdm(df.index)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw500.npy', 'wb'), protocol=4)\n",
        "    return data\n",
        "\n",
        "def load_raw_data_ptbxl(df, sampling_rate, path):\n",
        "    if sampling_rate == 100:\n",
        "        if os.path.exists(path + 'raw100.npy'):\n",
        "            data = np.load(path+'raw100.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path+f) for f in tqdm(df.filename_lr)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw100.npy', 'wb'), protocol=4)\n",
        "    elif sampling_rate == 500:\n",
        "        if os.path.exists(path + 'raw500.npy'):\n",
        "            data = np.load(path+'raw500.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path+f) for f in tqdm(df.filename_hr)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw500.npy', 'wb'), protocol=4)\n",
        "    return data\n",
        "\n",
        "def compute_label_aggregations(df, folder, ctype):\n",
        "\n",
        "    df['scp_codes_len'] = df.scp_codes.apply(lambda x: len(x))\n",
        "\n",
        "    aggregation_df = pd.read_csv(folder+'scp_statements.csv', index_col=0)\n",
        "\n",
        "    if ctype in ['diagnostic', 'subdiagnostic', 'superdiagnostic']:\n",
        "\n",
        "        def aggregate_all_diagnostic(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in diag_agg_df.index:\n",
        "                    tmp.append(key)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        def aggregate_subdiagnostic(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in diag_agg_df.index:\n",
        "                    c = diag_agg_df.loc[key].diagnostic_subclass\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        def aggregate_diagnostic(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in diag_agg_df.index:\n",
        "                    c = diag_agg_df.loc[key].diagnostic_class\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        diag_agg_df = aggregation_df[aggregation_df.diagnostic == 1.0]\n",
        "        if ctype == 'diagnostic':\n",
        "            df['diagnostic'] = df.scp_codes.apply(aggregate_all_diagnostic)\n",
        "            df['diagnostic_len'] = df.diagnostic.apply(lambda x: len(x))\n",
        "        elif ctype == 'subdiagnostic':\n",
        "            df['subdiagnostic'] = df.scp_codes.apply(aggregate_subdiagnostic)\n",
        "            df['subdiagnostic_len'] = df.subdiagnostic.apply(lambda x: len(x))\n",
        "        elif ctype == 'superdiagnostic':\n",
        "            df['superdiagnostic'] = df.scp_codes.apply(aggregate_diagnostic)\n",
        "            df['superdiagnostic_len'] = df.superdiagnostic.apply(lambda x: len(x))\n",
        "    elif ctype == 'form':\n",
        "        form_agg_df = aggregation_df[aggregation_df.form == 1.0]\n",
        "\n",
        "        def aggregate_form(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in form_agg_df.index:\n",
        "                    c = key\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        df['form'] = df.scp_codes.apply(aggregate_form)\n",
        "        df['form_len'] = df.form.apply(lambda x: len(x))\n",
        "    elif ctype == 'rhythm':\n",
        "        rhythm_agg_df = aggregation_df[aggregation_df.rhythm == 1.0]\n",
        "\n",
        "        def aggregate_rhythm(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in rhythm_agg_df.index:\n",
        "                    c = key\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        df['rhythm'] = df.scp_codes.apply(aggregate_rhythm)\n",
        "        df['rhythm_len'] = df.rhythm.apply(lambda x: len(x))\n",
        "    elif ctype == 'all':\n",
        "        df['all_scp'] = df.scp_codes.apply(lambda x: list(set(x.keys())))\n",
        "\n",
        "    return df\n",
        "\n",
        "def select_data(XX,YY, ctype, min_samples, outputfolder):\n",
        "    # convert multilabel to multi-hot\n",
        "    mlb = MultiLabelBinarizer()\n",
        "\n",
        "    if ctype == 'diagnostic':\n",
        "        X = XX[YY.diagnostic_len > 0]\n",
        "        Y = YY[YY.diagnostic_len > 0]\n",
        "        mlb.fit(Y.diagnostic.values)\n",
        "        y = mlb.transform(Y.diagnostic.values)\n",
        "    elif ctype == 'subdiagnostic':\n",
        "        counts = pd.Series(np.concatenate(YY.subdiagnostic.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.subdiagnostic = YY.subdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))\n",
        "        X = XX[YY.subdiagnostic_len > 0]\n",
        "        Y = YY[YY.subdiagnostic_len > 0]\n",
        "        mlb.fit(Y.subdiagnostic.values)\n",
        "        y = mlb.transform(Y.subdiagnostic.values)\n",
        "    elif ctype == 'superdiagnostic':\n",
        "        counts = pd.Series(np.concatenate(YY.superdiagnostic.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.superdiagnostic = YY.superdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['superdiagnostic_len'] = YY.superdiagnostic.apply(lambda x: len(x))\n",
        "        X = XX[YY.superdiagnostic_len > 0]\n",
        "        Y = YY[YY.superdiagnostic_len > 0]\n",
        "        mlb.fit(Y.superdiagnostic.values)\n",
        "        y = mlb.transform(Y.superdiagnostic.values)\n",
        "    elif ctype == 'form':\n",
        "        # filter\n",
        "        counts = pd.Series(np.concatenate(YY.form.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.form = YY.form.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['form_len'] = YY.form.apply(lambda x: len(x))\n",
        "        # select\n",
        "        X = XX[YY.form_len > 0]\n",
        "        Y = YY[YY.form_len > 0]\n",
        "        mlb.fit(Y.form.values)\n",
        "        y = mlb.transform(Y.form.values)\n",
        "    elif ctype == 'rhythm':\n",
        "        # filter \n",
        "        counts = pd.Series(np.concatenate(YY.rhythm.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.rhythm = YY.rhythm.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['rhythm_len'] = YY.rhythm.apply(lambda x: len(x))\n",
        "        # select\n",
        "        X = XX[YY.rhythm_len > 0]\n",
        "        Y = YY[YY.rhythm_len > 0]\n",
        "        mlb.fit(Y.rhythm.values)\n",
        "        y = mlb.transform(Y.rhythm.values)\n",
        "    elif ctype == 'all':\n",
        "        # filter \n",
        "        counts = pd.Series(np.concatenate(YY.all_scp.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.all_scp = YY.all_scp.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['all_scp_len'] = YY.all_scp.apply(lambda x: len(x))\n",
        "        # select\n",
        "        X = XX[YY.all_scp_len > 0]\n",
        "        Y = YY[YY.all_scp_len > 0]\n",
        "        mlb.fit(Y.all_scp.values)\n",
        "        y = mlb.transform(Y.all_scp.values)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    # save LabelBinarizer\n",
        "    with open(outputfolder+'mlb.pkl', 'wb') as tokenizer:\n",
        "        pickle.dump(mlb, tokenizer)\n",
        "\n",
        "    return X, Y, y, mlb\n",
        "\n",
        "def preprocess_signals(X_train, X_validation, X_test, outputfolder):\n",
        "    # Standardize data such that mean 0 and variance 1\n",
        "    ss = StandardScaler()\n",
        "    ss.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n",
        "    \n",
        "    # Save Standardizer data\n",
        "    with open(outputfolder+'standard_scaler.pkl', 'wb') as ss_file:\n",
        "        pickle.dump(ss, ss_file)\n",
        "\n",
        "    return apply_standardizer(X_train, ss), apply_standardizer(X_validation, ss), apply_standardizer(X_test, ss)\n",
        "\n",
        "def apply_standardizer(X, ss):\n",
        "    X_tmp = []\n",
        "    for x in X:\n",
        "        x_shape = x.shape\n",
        "        X_tmp.append(ss.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n",
        "    X_tmp = np.array(X_tmp)\n",
        "    return X_tmp\n",
        "\n",
        "\n",
        "# DOCUMENTATION STUFF\n",
        "\n",
        "def generate_ptbxl_summary_table(selection=None, folder='../output/'):\n",
        "\n",
        "    exps = ['exp0', 'exp1', 'exp1.1', 'exp1.1.1', 'exp2', 'exp3']\n",
        "    metric1 = 'macro_auc' \n",
        "\n",
        "    # get models\n",
        "    models = {}\n",
        "    for i, exp in enumerate(exps):\n",
        "        if selection is None:\n",
        "            exp_models = [m.split('/')[-1] for m in glob.glob(folder+str(exp)+'/models/*')]\n",
        "        else:\n",
        "            exp_models = selection\n",
        "        if i == 0:\n",
        "            models = set(exp_models)\n",
        "        else:\n",
        "            models = models.union(set(exp_models))\n",
        "\n",
        "    results_dic = {'Method':[], \n",
        "                'exp0_AUC':[], \n",
        "                'exp1_AUC':[], \n",
        "                'exp1.1_AUC':[], \n",
        "                'exp1.1.1_AUC':[], \n",
        "                'exp2_AUC':[],\n",
        "                'exp3_AUC':[]\n",
        "                }\n",
        "\n",
        "    for m in models:\n",
        "        results_dic['Method'].append(m)\n",
        "        \n",
        "        for e in exps:\n",
        "            \n",
        "            try:\n",
        "                me_res = pd.read_csv(folder+str(e)+'/models/'+str(m)+'/results/te_results.csv', index_col=0)\n",
        "    \n",
        "                mean1 = me_res.loc['point'][metric1]\n",
        "                unc1 = max(me_res.loc['upper'][metric1]-me_res.loc['point'][metric1], me_res.loc['point'][metric1]-me_res.loc['lower'][metric1])\n",
        "\n",
        "                results_dic[e+'_AUC'].append(\"%.3f(%.2d)\" %(np.round(mean1,3), int(unc1*1000)))\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                results_dic[e+'_AUC'].append(\"--\")\n",
        "            \n",
        "            \n",
        "    df = pd.DataFrame(results_dic)\n",
        "    df_index = df[df.Method.isin(['naive', 'ensemble'])]\n",
        "    df_rest = df[~df.Method.isin(['naive', 'ensemble'])]\n",
        "    df = pd.concat([df_rest, df_index])\n",
        "    df.to_csv(folder+'results_ptbxl.csv')\n",
        "\n",
        "    titles = [\n",
        "        '### 1. PTB-XL: all statements',\n",
        "        '### 2. PTB-XL: diagnostic statements',\n",
        "        '### 3. PTB-XL: Diagnostic subclasses',\n",
        "        '### 4. PTB-XL: Diagnostic superclasses',\n",
        "        '### 5. PTB-XL: Form statements',\n",
        "        '### 6. PTB-XL: Rhythm statements'        \n",
        "    ]\n",
        "\n",
        "    # helper output function for markdown tables\n",
        "    our_work = 'https://arxiv.org/abs/2004.13701'\n",
        "    our_repo = 'https://github.com/helme/ecg_ptbxl_benchmarking/'\n",
        "    md_source = ''\n",
        "    for i, e in enumerate(exps):\n",
        "        md_source += '\\n '+titles[i]+' \\n \\n'\n",
        "        md_source += '| Model | AUC &darr; | paper/source | code | \\n'\n",
        "        md_source += '|---:|:---|:---|:---| \\n'\n",
        "        for row in df_rest[['Method', e+'_AUC']].sort_values(e+'_AUC', ascending=False).values:\n",
        "            md_source += '| ' + row[0].replace('fastai_', '') + ' | ' + row[1] + ' | [our work]('+our_work+') | [this repo]('+our_repo+')| \\n'\n",
        "    print(md_source)\n",
        "\n",
        "def ICBEBE_table(selection=None, folder='../output/'):\n",
        "    cols = ['macro_auc', 'F_beta_macro', 'G_beta_macro']\n",
        "\n",
        "    if selection is None:\n",
        "        models = [m.split('/')[-1].split('_pretrained')[0] for m in glob.glob(folder+'exp_ICBEB/models/*')]\n",
        "    else:\n",
        "        models = [] \n",
        "        for s in selection:\n",
        "            #if s != 'Wavelet+NN':\n",
        "                models.append(s)\n",
        "\n",
        "    data = []\n",
        "    for model in models:\n",
        "        me_res = pd.read_csv(folder+'exp_ICBEB/models/'+model+'/results/te_results.csv', index_col=0)\n",
        "        mcol=[]\n",
        "        for col in cols:\n",
        "            mean = me_res.ix['point'][col]\n",
        "            unc = max(me_res.ix['upper'][col]-me_res.ix['point'][col], me_res.ix['point'][col]-me_res.ix['lower'][col])\n",
        "            mcol.append(\"%.3f(%.2d)\" %(np.round(mean,3), int(unc*1000)))\n",
        "        data.append(mcol)\n",
        "    data = np.array(data)\n",
        "\n",
        "    df = pd.DataFrame(data, columns=cols, index=models)\n",
        "    df.to_csv(folder+'results_icbeb.csv')\n",
        "\n",
        "    df_rest = df[~df.index.isin(['naive', 'ensemble'])]\n",
        "    df_rest = df_rest.sort_values('macro_auc', ascending=False)\n",
        "    our_work = 'https://arxiv.org/abs/2004.13701'\n",
        "    our_repo = 'https://github.com/helme/ecg_ptbxl_benchmarking/'\n",
        "\n",
        "    md_source = '| Model | AUC &darr; |  F_beta=2 | G_beta=2 | paper/source | code | \\n'\n",
        "    md_source += '|---:|:---|:---|:---|:---|:---| \\n'\n",
        "    for i, row in enumerate(df_rest[cols].values):\n",
        "        md_source += '| ' + df_rest.index[i].replace('fastai_', '') + ' | ' + row[0] + ' | ' + row[1] + ' | ' + row[2] + ' | [our work]('+our_work+') | [this repo]('+our_repo+')| \\n'\n",
        "    print(md_source)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO4FB4Qbxrfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc842569-c26a-4fdd-ba88-ce1e184f8ebb"
      },
      "source": [
        "from utils import utils\n",
        "\n",
        "sampling_frequency=100\n",
        "datafolder= '/gdrive/My Drive/ICBEB/'\n",
        "task='all'\n",
        "outputfolder='../output0/'\n",
        "\n",
        "# Load data\n",
        "data, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
        "# Preprocess label data\n",
        "labels = utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
        "# Select relevant data and convert to one-hot\n",
        "data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
        "\n",
        "# 1-9 for training \n",
        "X_train = data[labels.strat_fold < 10]\n",
        "y_train = Y[labels.strat_fold < 10]\n",
        "# 10 for validation\n",
        "X_val = data[labels.strat_fold == 10]\n",
        "y_val = Y[labels.strat_fold == 10]\n",
        "\n",
        "num_classes = 9         # <=== number of classes in the finetuning dataset\n",
        "input_shape = [1000,12] # <=== shape of samples, [None, 12] in case of different lengths\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6187,), (6187, 9), (690,), (690, 9))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtV-zMEK1GOb",
        "outputId": "6e1ed5f7-18f6-4f80-ec77-a2148157032d"
      },
      "source": [
        "X_tr = []\n",
        "for i in range(len(X_train)):\n",
        "    x = []\n",
        "    for j in range(12):\n",
        "        p = X_train[i][:1000,j]\n",
        "        if p.shape[0]!= 1000:\n",
        "            d = abs(p.shape[0]-1000)//2\n",
        "            p = np.pad(p,(d,d))\n",
        "        x.append(p)\n",
        "    X_tr.append(np.transpose(x))\n",
        "X_tr = np.array(X_tr)\n",
        "X_tr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6187, 1000, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qpz5wPKJOxS",
        "outputId": "8b155773-79b2-4fd7-829e-7c38599c3b3e"
      },
      "source": [
        "X_te = []\n",
        "for i in range(len(X_val)):\n",
        "    x = []\n",
        "    for j in range(12):\n",
        "        p = X_val[i][:1000,j]\n",
        "        if p.shape[0]!= 1000:\n",
        "            d = abs(p.shape[0]-1000)//2\n",
        "            p = np.pad(p,(d,d))\n",
        "        x.append(p)\n",
        "    X_te.append(np.transpose(x))\n",
        "X_te = np.array(X_te)\n",
        "X_te.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(690, 1000, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFdluV6Ic2zE",
        "outputId": "e65fe156-1e37-4b01-b002-a4a009f005fa"
      },
      "source": [
        "cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR7XSXPaeJX0"
      },
      "source": [
        "Model Definition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNixRK2WeM1v",
        "outputId": "da9b4ea7-af43-475a-a880-5a8221c50898"
      },
      "source": [
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense)\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ResidualUnit(object):\n",
        "    \"\"\"Residual unit block (unidimensional).\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples_out: int\n",
        "        Number of output samples.\n",
        "    n_filters_out: int\n",
        "        Number of output filters.\n",
        "    kernel_initializer: str, optional\n",
        "        Initializer for the weights matrices. See Keras initializers. By default it uses\n",
        "        'he_normal'.\n",
        "    dropout_keep_prob: float [0, 1), optional\n",
        "        Dropout rate used in all Dropout layers. Default is 0.8\n",
        "    kernel_size: int, optional\n",
        "        Kernel size for convolutional layers. Default is 17.\n",
        "    preactivation: bool, optional\n",
        "        When preactivation is true use full preactivation architecture proposed\n",
        "        in [1]. Otherwise, use architecture proposed in the original ResNet\n",
        "        paper [2]. By default it is true.\n",
        "    postactivation_bn: bool, optional\n",
        "        Defines if you use batch normalization before or after the activation layer (there\n",
        "        seems to be some advantages in some cases:\n",
        "        https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md).\n",
        "        If true, the batch normalization is used before the activation\n",
        "        function, otherwise the activation comes first, as it is usually done.\n",
        "        By default it is false.\n",
        "    activation_function: string, optional\n",
        "        Keras activation function to be used. By default 'relu'.\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] K. He, X. Zhang, S. Ren, and J. Sun, \"Identity Mappings in Deep Residual Networks,\"\n",
        "           arXiv:1603.05027 [cs], Mar. 2016. https://arxiv.org/pdf/1603.05027.pdf.\n",
        "    .. [2] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" in 2016 IEEE Conference\n",
        "           on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778. https://arxiv.org/pdf/1512.03385.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_samples_out, n_filters_out, kernel_initializer='he_normal',\n",
        "                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n",
        "                 postactivation_bn=False, activation_function='relu'):\n",
        "        self.n_samples_out = n_samples_out\n",
        "        self.n_filters_out = n_filters_out\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.dropout_rate = 1 - dropout_keep_prob\n",
        "        self.kernel_size = kernel_size\n",
        "        self.preactivation = preactivation\n",
        "        self.postactivation_bn = postactivation_bn\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def _skip_connection(self, y, downsample, n_filters_in):\n",
        "        \"\"\"Implement skip connection.\"\"\"\n",
        "        # Deal with downsampling\n",
        "        if downsample > 1:\n",
        "            y = MaxPooling1D(downsample, strides=downsample, padding='same')(y)\n",
        "        elif downsample == 1:\n",
        "            y = y\n",
        "        else:\n",
        "            raise ValueError(\"Number of samples should always decrease.\")\n",
        "        # Deal with n_filters dimension increase\n",
        "        if n_filters_in != self.n_filters_out:\n",
        "            # This is one of the two alternatives presented in ResNet paper\n",
        "            # Other option is to just fill the matrix with zeros.\n",
        "            y = Conv1D(self.n_filters_out, 1, padding='same',\n",
        "                       use_bias=False, kernel_initializer=self.kernel_initializer)(y)\n",
        "        return y\n",
        "\n",
        "    def _batch_norm_plus_activation(self, x):\n",
        "        if self.postactivation_bn:\n",
        "            x = Activation(self.activation_function)(x)\n",
        "            x = BatchNormalization(center=False, scale=False)(x)\n",
        "        else:\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation(self.activation_function)(x)\n",
        "        return x\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"Residual unit.\"\"\"\n",
        "        x, y = inputs\n",
        "        n_samples_in = y.shape[1]\n",
        "        downsample = n_samples_in // self.n_samples_out\n",
        "        n_filters_in = y.shape[2]\n",
        "        y = self._skip_connection(y, downsample, n_filters_in)\n",
        "        # 1st layer\n",
        "        x = Conv1D(self.n_filters_out, self.kernel_size, padding='same',\n",
        "                   use_bias=False, kernel_initializer=self.kernel_initializer)(x)\n",
        "        x = self._batch_norm_plus_activation(x)\n",
        "        if self.dropout_rate > 0:\n",
        "            x = Dropout(self.dropout_rate)(x)\n",
        "\n",
        "        # 2nd layer\n",
        "        x = Conv1D(self.n_filters_out, self.kernel_size, strides=downsample,\n",
        "                   padding='same', use_bias=False,\n",
        "                   kernel_initializer=self.kernel_initializer)(x)\n",
        "        if self.preactivation:\n",
        "            x = Add()([x, y])  # Sum skip connection and main connection\n",
        "            y = x\n",
        "            x = self._batch_norm_plus_activation(x)\n",
        "            if self.dropout_rate > 0:\n",
        "                x = Dropout(self.dropout_rate)(x)\n",
        "        else:\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Add()([x, y])  # Sum skip connection and main connection\n",
        "            x = Activation(self.activation_function)(x)\n",
        "            if self.dropout_rate > 0:\n",
        "                x = Dropout(self.dropout_rate)(x)\n",
        "            y = x\n",
        "        return [x, y]\n",
        "\n",
        "\n",
        "def get_model(n_classes, last_layer='sigmoid'):\n",
        "    kernel_size = 16\n",
        "    kernel_initializer = 'he_normal'\n",
        "    signal = Input(shape=(1000, 12), dtype=np.float32, name='signal')\n",
        "    x = signal\n",
        "    x = Conv1D(8, kernel_size, padding='same', use_bias=False,\n",
        "               kernel_initializer=kernel_initializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x, y = ResidualUnit(128, 8, kernel_size=kernel_size,\n",
        "                        kernel_initializer=kernel_initializer)([x, x])\n",
        "    x, y = ResidualUnit(64, 16, kernel_size=kernel_size,\n",
        "                        kernel_initializer=kernel_initializer)([x, y])\n",
        "    x, y = ResidualUnit(32, 32, kernel_size=kernel_size,\n",
        "                        kernel_initializer=kernel_initializer)([x, y])\n",
        "    x, _ = ResidualUnit(16, 64, kernel_size=kernel_size,\n",
        "                        kernel_initializer=kernel_initializer)([x, y])\n",
        "    x = Flatten()(x)\n",
        "    diagn = Dense(9, activation=last_layer, kernel_initializer=kernel_initializer)(x)\n",
        "    model = Model(signal, diagn)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = get_model(6)\n",
        "    model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "signal (InputLayer)             [(None, 1000, 12)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 1000, 8)      1536        signal[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 1000, 8)      32          conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 1000, 8)      0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1000, 8)      1024        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1000, 8)      32          conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1000, 8)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1000, 8)      0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 143, 8)       1024        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 143, 8)       0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 143, 8)       0           conv1d_2[0][0]                   \n",
            "                                                                 max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 143, 8)       32          add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 143, 8)       0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 143, 8)       0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 143, 16)      2048        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 143, 16)      64          conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 143, 16)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 143, 16)      0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 72, 8)        0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 72, 16)       4096        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 72, 16)       128         max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 72, 16)       0           conv1d_5[0][0]                   \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 72, 16)       64          add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 72, 16)       0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 72, 16)       0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 72, 32)       8192        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 72, 32)       128         conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 72, 32)       0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 72, 32)       0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 36, 16)       0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 36, 32)       16384       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 36, 32)       512         max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 36, 32)       0           conv1d_8[0][0]                   \n",
            "                                                                 conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 36, 32)       128         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 36, 32)       0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 36, 32)       0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 36, 64)       32768       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 36, 64)       256         conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 36, 64)       0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 36, 64)       0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 18, 32)       0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 18, 64)       65536       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 18, 64)       2048        max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 18, 64)       0           conv1d_11[0][0]                  \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 18, 64)       256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 18, 64)       0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 18, 64)       0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 9)            10377       flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 146,665\n",
            "Trainable params: 146,169\n",
            "Non-trainable params: 496\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CYfi1HGdlWH"
      },
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXp8Zkd2N3DT"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,\n",
        "                                        CSVLogger, EarlyStopping)\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, fbeta_score, roc_auc_score, roc_curve, roc_curve, auc , f1_score\n",
        "# from model import get_model\n",
        "import argparse\n",
        "# from datasets import ECGSequence\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import mlflow\n",
        "import mlflow.tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDZxAcWTvEOk"
      },
      "source": [
        "mlflow.tensorflow.autolog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxprOiQxvKYO",
        "outputId": "ed0fea98-1577-4259-db8f-a2a5086b214f"
      },
      "source": [
        "with mlflow.start_run() as run:\n",
        "\n",
        "    def find_optimal_cutoff_threshold(target, predicted):\n",
        "        \"\"\" \n",
        "        Find the optimal probability cutoff point for a classification model related to event rate\n",
        "        \"\"\"\n",
        "        fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "        optimal_idx = np.argmax(tpr - fpr)\n",
        "        optimal_threshold = threshold[optimal_idx]\n",
        "        return optimal_threshold\n",
        "\n",
        "    def find_optimal_cutoff_thresholds(y_true, y_pred):\n",
        "\t    return [find_optimal_cutoff_threshold(y_true[:,i], y_pred[:,i]) for i in range(y_true.shape[1])]\n",
        "\n",
        "    def apply_thresholds(preds, thresholds):\n",
        "        \"\"\"\n",
        "            apply class-wise thresholds to prediction score in order to get binary format.\n",
        "            BUT: if no score is above threshold, pick maximum. This is needed due to metric issues.\n",
        "        \"\"\"\n",
        "        tmp = []\n",
        "        for p in preds:\n",
        "            tmp_p = (p > thresholds).astype(int)\n",
        "            if np.sum(tmp_p) == 0:\n",
        "                tmp_p[np.argmax(p)] = 1\n",
        "            tmp.append(tmp_p)\n",
        "        tmp = np.array(tmp)\n",
        "        return tmp\n",
        "\n",
        "    def step_decay(epoch):\n",
        "        initial_lrate = 0.001\n",
        "        drop = 0.4\n",
        "        epochs_drop = 20.0\n",
        "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "        return lrate\n",
        "\n",
        "    # lscheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "    lr = 0.001\n",
        "    callbacks = [ReduceLROnPlateau(monitor='val_loss',\n",
        "                                    factor=0.1,\n",
        "                                    patience=7,\n",
        "                                    min_lr=lr / 100),\n",
        "                    EarlyStopping(patience=9,  # Patience should be larger than the one in ReduceLROnPlateau\n",
        "                                min_delta=0.00001)]\n",
        "\n",
        "    adam = tf.keras.optimizers.Adam(lr)\n",
        "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['AUC'])\n",
        "\n",
        "    num_folds = 5\n",
        "    epochs = 20\n",
        "    batch_size= 32\n",
        "    kfold = KFold(num_folds)\n",
        "\n",
        "    histories = []\n",
        "    fold_no = 1\n",
        "    for train, test in kfold.split(X_tr, y_train):\n",
        "\n",
        "        h = model.fit(X_tr[train], y_train[train],\n",
        "                        epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=(X_tr[test],y_train[test]),\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "        print(\"iteration \", str(fold_no))\n",
        "        histories.append(h)\n",
        "        fold_no += 1\n",
        "\n",
        "    # predictions = model.predict(X_te)\n",
        "    test_auc = model.evaluate(X_te, y_val)[1]\n",
        "    print(\"Test auc avg: \", test_auc)\n",
        "\n",
        "    y_pred = model.predict(X_te)\n",
        "    thresholds =  find_optimal_cutoff_thresholds_for_Gbeta(y_val , y_pred)\n",
        "    y_pred_binary = apply_thresholds(y_pred, thresholds)\n",
        "\n",
        "    f1 = f1_score(y_val, y_pred_binary, average='macro')\n",
        "\n",
        "    mlflow.log_metric(\"Test AUC\", test_auc) \n",
        "    mlflow.log_metric('macro_f1_score' , f1) \n",
        "\n",
        "    mlflow.keras.log_model(model, \"nature_model\")\n",
        "                        #    custom_objects=\n",
        "                        #    {\"log_cosh_dice_loss\": log_cosh_dice_loss,\"dice_coef\":dice_coef})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "155/155 [==============================] - 27s 146ms/step - loss: 0.3328 - auc: 0.7320 - val_loss: 0.2993 - val_auc: 0.8130\n",
            "Epoch 2/20\n",
            "155/155 [==============================] - 21s 135ms/step - loss: 0.2576 - auc: 0.8509 - val_loss: 0.2576 - val_auc: 0.8654\n",
            "Epoch 3/20\n",
            "155/155 [==============================] - 20s 132ms/step - loss: 0.2303 - auc: 0.8868 - val_loss: 0.2753 - val_auc: 0.8674\n",
            "Epoch 4/20\n",
            "155/155 [==============================] - 21s 134ms/step - loss: 0.2094 - auc: 0.9086 - val_loss: 0.2770 - val_auc: 0.8685\n",
            "Epoch 5/20\n",
            "155/155 [==============================] - 21s 133ms/step - loss: 0.1892 - auc: 0.9276 - val_loss: 0.1977 - val_auc: 0.9294\n",
            "Epoch 6/20\n",
            "155/155 [==============================] - 21s 137ms/step - loss: 0.1770 - auc: 0.9382 - val_loss: 0.1734 - val_auc: 0.9426\n",
            "Epoch 7/20\n",
            "155/155 [==============================] - 21s 135ms/step - loss: 0.1616 - auc: 0.9488 - val_loss: 0.1745 - val_auc: 0.9442\n",
            "Epoch 8/20\n",
            "155/155 [==============================] - 21s 134ms/step - loss: 0.1501 - auc: 0.9562 - val_loss: 0.1798 - val_auc: 0.9396\n",
            "Epoch 9/20\n",
            "155/155 [==============================] - 20s 130ms/step - loss: 0.1436 - auc: 0.9605 - val_loss: 0.1988 - val_auc: 0.9358\n",
            "Epoch 10/20\n",
            "155/155 [==============================] - 21s 138ms/step - loss: 0.1345 - auc: 0.9663 - val_loss: 0.1980 - val_auc: 0.9415\n",
            "Epoch 11/20\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 0.1279 - auc: 0.9699 - val_loss: 0.2008 - val_auc: 0.9399\n",
            "Epoch 12/20\n",
            "155/155 [==============================] - 23s 145ms/step - loss: 0.1236 - auc: 0.9717 - val_loss: 0.1863 - val_auc: 0.9427\n",
            "Epoch 13/20\n",
            "155/155 [==============================] - 21s 134ms/step - loss: 0.1194 - auc: 0.9741 - val_loss: 0.1763 - val_auc: 0.9488\n",
            "Epoch 14/20\n",
            "155/155 [==============================] - 21s 134ms/step - loss: 0.1030 - auc: 0.9820 - val_loss: 0.1530 - val_auc: 0.9590\n",
            "Epoch 15/20\n",
            "155/155 [==============================] - 20s 132ms/step - loss: 0.0972 - auc: 0.9840 - val_loss: 0.1496 - val_auc: 0.9605\n",
            "Epoch 16/20\n",
            "155/155 [==============================] - 21s 136ms/step - loss: 0.0976 - auc: 0.9841 - val_loss: 0.1499 - val_auc: 0.9604\n",
            "Epoch 17/20\n",
            "155/155 [==============================] - 21s 136ms/step - loss: 0.0956 - auc: 0.9851 - val_loss: 0.1518 - val_auc: 0.9610\n",
            "Epoch 18/20\n",
            "155/155 [==============================] - 21s 137ms/step - loss: 0.0926 - auc: 0.9855 - val_loss: 0.1512 - val_auc: 0.9597\n",
            "Epoch 19/20\n",
            "155/155 [==============================] - 21s 135ms/step - loss: 0.0938 - auc: 0.9855 - val_loss: 0.1541 - val_auc: 0.9598\n",
            "Epoch 20/20\n",
            "155/155 [==============================] - 20s 132ms/step - loss: 0.0913 - auc: 0.9861 - val_loss: 0.1515 - val_auc: 0.9605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp2bpv2tli/model/data/model/assets\n",
            "iteration  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021/08/10 04:53:54 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='21e68a9132b0469e940144868621bb09'. Attempted logging new value '0.000100000005'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "155/155 [==============================] - 23s 151ms/step - loss: 0.1106 - auc: 0.9769 - val_loss: 0.0719 - val_auc: 0.9919\n",
            "Epoch 2/20\n",
            "155/155 [==============================] - 25s 159ms/step - loss: 0.1068 - auc: 0.9786 - val_loss: 0.0715 - val_auc: 0.9920\n",
            "Epoch 3/20\n",
            "155/155 [==============================] - 24s 155ms/step - loss: 0.1066 - auc: 0.9793 - val_loss: 0.0721 - val_auc: 0.9918\n",
            "Epoch 4/20\n",
            "155/155 [==============================] - 27s 174ms/step - loss: 0.1045 - auc: 0.9797 - val_loss: 0.0733 - val_auc: 0.9916\n",
            "Epoch 5/20\n",
            "155/155 [==============================] - 28s 180ms/step - loss: 0.1030 - auc: 0.9802 - val_loss: 0.0728 - val_auc: 0.9916\n",
            "Epoch 6/20\n",
            "155/155 [==============================] - 26s 168ms/step - loss: 0.1025 - auc: 0.9808 - val_loss: 0.0757 - val_auc: 0.9909\n",
            "Epoch 7/20\n",
            "155/155 [==============================] - 26s 170ms/step - loss: 0.1025 - auc: 0.9797 - val_loss: 0.0750 - val_auc: 0.9909\n",
            "Epoch 8/20\n",
            "155/155 [==============================] - 27s 176ms/step - loss: 0.0996 - auc: 0.9814 - val_loss: 0.0747 - val_auc: 0.9910\n",
            "Epoch 9/20\n",
            "155/155 [==============================] - 26s 170ms/step - loss: 0.0989 - auc: 0.9825 - val_loss: 0.0742 - val_auc: 0.9911\n",
            "Epoch 10/20\n",
            "155/155 [==============================] - 27s 173ms/step - loss: 0.0973 - auc: 0.9824 - val_loss: 0.0750 - val_auc: 0.9908\n",
            "Epoch 11/20\n",
            "155/155 [==============================] - 26s 170ms/step - loss: 0.0975 - auc: 0.9823 - val_loss: 0.0752 - val_auc: 0.9908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpd3lcuua5/model/data/model/assets\n",
            "iteration  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021/08/10 04:59:17 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='21e68a9132b0469e940144868621bb09'. Attempted logging new value '1.0000001e-05'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "155/155 [==============================] - 24s 158ms/step - loss: 0.1024 - auc: 0.9808 - val_loss: 0.0541 - val_auc: 0.9954\n",
            "Epoch 2/20\n",
            "155/155 [==============================] - 25s 159ms/step - loss: 0.1027 - auc: 0.9805 - val_loss: 0.0549 - val_auc: 0.9953\n",
            "Epoch 3/20\n",
            "155/155 [==============================] - 23s 149ms/step - loss: 0.1012 - auc: 0.9818 - val_loss: 0.0549 - val_auc: 0.9953\n",
            "Epoch 4/20\n",
            "155/155 [==============================] - 23s 151ms/step - loss: 0.0989 - auc: 0.9822 - val_loss: 0.0550 - val_auc: 0.9953\n",
            "Epoch 5/20\n",
            "155/155 [==============================] - 25s 160ms/step - loss: 0.1014 - auc: 0.9817 - val_loss: 0.0547 - val_auc: 0.9953\n",
            "Epoch 6/20\n",
            "155/155 [==============================] - 23s 149ms/step - loss: 0.1023 - auc: 0.9807 - val_loss: 0.0551 - val_auc: 0.9953\n",
            "Epoch 7/20\n",
            "155/155 [==============================] - 23s 147ms/step - loss: 0.1004 - auc: 0.9813 - val_loss: 0.0550 - val_auc: 0.9952\n",
            "Epoch 8/20\n",
            "155/155 [==============================] - 25s 159ms/step - loss: 0.0999 - auc: 0.9818 - val_loss: 0.0557 - val_auc: 0.9951\n",
            "Epoch 9/20\n",
            "155/155 [==============================] - 24s 157ms/step - loss: 0.0992 - auc: 0.9819 - val_loss: 0.0554 - val_auc: 0.9952\n",
            "Epoch 10/20\n",
            "155/155 [==============================] - 24s 152ms/step - loss: 0.1000 - auc: 0.9817 - val_loss: 0.0557 - val_auc: 0.9951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpsz5orbos/model/data/model/assets\n",
            "iteration  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021/08/10 05:03:27 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='21e68a9132b0469e940144868621bb09'. Attempted logging new value '1e-05'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 0.0983 - auc: 0.9822 - val_loss: 0.0632 - val_auc: 0.9945\n",
            "Epoch 2/20\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 0.0982 - auc: 0.9820 - val_loss: 0.0643 - val_auc: 0.9943\n",
            "Epoch 3/20\n",
            "155/155 [==============================] - 21s 138ms/step - loss: 0.0976 - auc: 0.9823 - val_loss: 0.0643 - val_auc: 0.9943\n",
            "Epoch 4/20\n",
            "155/155 [==============================] - 23s 147ms/step - loss: 0.0979 - auc: 0.9822 - val_loss: 0.0641 - val_auc: 0.9942\n",
            "Epoch 5/20\n",
            "155/155 [==============================] - 23s 147ms/step - loss: 0.0972 - auc: 0.9827 - val_loss: 0.0643 - val_auc: 0.9941\n",
            "Epoch 6/20\n",
            "155/155 [==============================] - 22s 144ms/step - loss: 0.0984 - auc: 0.9820 - val_loss: 0.0638 - val_auc: 0.9943\n",
            "Epoch 7/20\n",
            "155/155 [==============================] - 22s 140ms/step - loss: 0.0958 - auc: 0.9828 - val_loss: 0.0639 - val_auc: 0.9942\n",
            "Epoch 8/20\n",
            "155/155 [==============================] - 22s 144ms/step - loss: 0.0992 - auc: 0.9818 - val_loss: 0.0647 - val_auc: 0.9941\n",
            "Epoch 9/20\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 0.0964 - auc: 0.9830 - val_loss: 0.0643 - val_auc: 0.9941\n",
            "Epoch 10/20\n",
            "155/155 [==============================] - 23s 148ms/step - loss: 0.0952 - auc: 0.9836 - val_loss: 0.0647 - val_auc: 0.9941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpc83x4d_8/model/data/model/assets\n",
            "iteration  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021/08/10 05:07:50 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='21e68a9132b0469e940144868621bb09'. Attempted logging new value '1e-05'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "155/155 [==============================] - 24s 154ms/step - loss: 0.0961 - auc: 0.9826 - val_loss: 0.0567 - val_auc: 0.9953\n",
            "Epoch 2/20\n",
            "155/155 [==============================] - 22s 141ms/step - loss: 0.0991 - auc: 0.9817 - val_loss: 0.0569 - val_auc: 0.9953\n",
            "Epoch 3/20\n",
            "155/155 [==============================] - 22s 140ms/step - loss: 0.0992 - auc: 0.9817 - val_loss: 0.0574 - val_auc: 0.9952\n",
            "Epoch 4/20\n",
            "155/155 [==============================] - 23s 146ms/step - loss: 0.0977 - auc: 0.9830 - val_loss: 0.0574 - val_auc: 0.9951\n",
            "Epoch 5/20\n",
            "155/155 [==============================] - 22s 144ms/step - loss: 0.0987 - auc: 0.9818 - val_loss: 0.0575 - val_auc: 0.9952\n",
            "Epoch 6/20\n",
            "155/155 [==============================] - 23s 148ms/step - loss: 0.0970 - auc: 0.9833 - val_loss: 0.0575 - val_auc: 0.9951\n",
            "Epoch 7/20\n",
            "155/155 [==============================] - 23s 148ms/step - loss: 0.0963 - auc: 0.9833 - val_loss: 0.0577 - val_auc: 0.9951\n",
            "Epoch 8/20\n",
            "155/155 [==============================] - 23s 150ms/step - loss: 0.0970 - auc: 0.9832 - val_loss: 0.0577 - val_auc: 0.9951\n",
            "Epoch 9/20\n",
            "155/155 [==============================] - 21s 138ms/step - loss: 0.0971 - auc: 0.9825 - val_loss: 0.0581 - val_auc: 0.9950\n",
            "Epoch 10/20\n",
            "155/155 [==============================] - 22s 142ms/step - loss: 0.0972 - auc: 0.9825 - val_loss: 0.0587 - val_auc: 0.9949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpryrtlgr6/model/data/model/assets\n",
            "iteration  5\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.1579 - auc: 0.9571\n",
            "Test auc avg:  0.9571318626403809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "optimize thresholds with respect to G_beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:08<00:00,  1.08it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmplx__v1o6/model/data/model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFzK4oq94GlW",
        "outputId": "6ae4dc93-8a8a-44f1-e476-280fd9a17ef7"
      },
      "source": [
        " model.evaluate(X_te, y_val)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 1s 30ms/step - loss: 0.1579 - auc: 0.9571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9571318626403809"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzXjfvp6zo-4",
        "outputId": "204734bd-aa60-4470-e86f-279ea061c8e3"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.0.6.tar.gz (746 kB)\n",
            "\u001b[K     |████████████████████████████████| 746 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (5.4.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.0.6-py3-none-any.whl size=19263 sha256=e4f925a541939c8d8a08ba4c69f19d42516f01b8be93ce13026378ff88d12a11\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/8c/c4/8d9cbca4fa19bf64887b4a91914194bb9033f1a7cbb344d5ab\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwU2yLuGGjRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6397231-cff8-46c6-ad8a-b996b7dab1ed"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"1w9tvYSl88onvrzvZJMxMcDDb9Y_3NHGDZssJWJ6rdcw9TsBN\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLflow Tracking UI: https://3780d198fd00.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvTbEaOSK2LR",
        "outputId": "7bd6959e-9441-42ca-e6ac-248a55bc7649"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"1w9tvYSl88onvrzvZJMxMcDDb9Y_3NHGDZssJWJ6rdcw9TsBN\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLflow Tracking UI: https://a3de6dc54841.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIS0K7l_O_yx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}