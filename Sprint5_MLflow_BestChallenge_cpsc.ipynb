{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint5: MLflow_BestChallenge_cpsc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-torki/ECG-Classification/blob/main/Sprint5_MLflow_BestChallenge_cpsc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQhcIBu7oGKn"
      },
      "source": [
        "This notebook is based on the paper: \n",
        "\n",
        "**[Deep Learning for ECG Analysis: Benchmarks and Insights from PTB-XL](https://ieeexplore.ieee.org/document/9190034)**\n",
        "\n",
        "link to [PCSC-2018](http://2018.icbeb.org/Challenge.html) database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wORp-jjQCMX-",
        "outputId": "7c76aa84-9c11-4318-fb0e-468400b72d92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_D850nzCiSq",
        "outputId": "9c41258d-9c0b-482f-c742-0a3e81f17b02"
      },
      "source": [
        "cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1144sRmJVHVx",
        "outputId": "49f829fe-2bf0-4d73-ffdb-116acd116e17"
      },
      "source": [
        "!pip install wfdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-3.4.0-py3-none-any.whl (137 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 92 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 102 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 112 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 122 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 133 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 137 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.10.0)\n",
            "Collecting threadpoolctl>=1.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: urllib3>=1.22 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.24.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from wfdb) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.0.1)\n",
            "Requirement already satisfied: idna>=2.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.1.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.7/dist-packages (from wfdb) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2016.8.2 in /usr/local/lib/python3.7/dist-packages (from wfdb) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10.0->wfdb) (1.15.0)\n",
            "Installing collected packages: threadpoolctl, wfdb\n",
            "Successfully installed threadpoolctl-2.2.0 wfdb-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRb5ilbsP9Hz",
        "outputId": "80a3dfb9-00d7-41ec-bc58-97af60435ca3"
      },
      "source": [
        "!pip install mlflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-1.19.0-py3-none-any.whl (14.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.4 MB 63 kB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (21.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2018.9)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.15.0.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 25.5 MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 60.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic<=1.4.1->mlflow) (2.8.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.1.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (4.6.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow) (57.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->mlflow) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (2.4.7)\n",
            "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n",
            "Building wheels for collected packages: alembic, databricks-cli, prometheus-flask-exporter\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158170 sha256=15dab5de3dcf6d7eb2d8a248689b9039f8650b51b33ef1eb75eb98e41bb45b0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.15.0-py3-none-any.whl size=105259 sha256=5e31664497b100b77dd81f85efa1dfd073b166a4f7a7f8c643afa5d9b269f113\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/ba/75/284f9a90ff7a010bb23b9798f2e9a19dd9fe619379c917bff4\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17416 sha256=81221a83c1b384a0c0b563e7db0857c30e07fe3df69e6f6a94c7c35257101c12\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/1e/1c/c765920cb92b2f0343d2dd8b481a407cee2823f9b4bbd2e52a\n",
            "Successfully built alembic databricks-cli prometheus-flask-exporter\n",
            "Installing collected packages: smmap, websocket-client, python-editor, Mako, gitdb, querystring-parser, pyyaml, prometheus-flask-exporter, gunicorn, gitpython, docker, databricks-cli, alembic, mlflow\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.1.4 alembic-1.4.1 databricks-cli-0.15.0 docker-5.0.0 gitdb-4.0.7 gitpython-3.1.18 gunicorn-20.1.0 mlflow-1.19.0 prometheus-flask-exporter-0.18.2 python-editor-1.0.4 pyyaml-5.4.1 querystring-parser-1.2.4 smmap-4.0.0 websocket-client-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amteMY4fRNlj"
      },
      "source": [
        "# !git clone https://github.com/helme/ecg_ptbxl_benchmarking/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ig75yuxRWEE",
        "outputId": "5cb2b40e-ea84-4529-f57f-160be4191832"
      },
      "source": [
        "cd ./ecg_ptbxl_benchmarking/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/ecg_ptbxl_benchmarking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ytFYZfLRnMp",
        "outputId": "baae1d6e-8c64-4382-a3e2-18809a324eb1"
      },
      "source": [
        "cd code/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/ecg_ptbxl_benchmarking/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3qSI1C11wkq",
        "cellView": "form"
      },
      "source": [
        "#@title utils\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import glob\n",
        "import pickle\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import wfdb\n",
        "import ast\n",
        "from sklearn.metrics import classification_report, fbeta_score, roc_auc_score, roc_curve, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
        "import warnings\n",
        "\n",
        "# EVALUATION STUFF\n",
        "def generate_results(idxs, y_true, y_pred, thresholds):\n",
        "    return evaluate_experiment(y_true[idxs], y_pred[idxs], thresholds)\n",
        "\n",
        "def evaluate_experiment(y_true, y_pred, thresholds=None):\n",
        "    results = {}\n",
        "\n",
        "    if not thresholds is None:\n",
        "        # binary predictions\n",
        "        y_pred_binary = apply_thresholds(y_pred, thresholds)\n",
        "        # PhysioNet/CinC Challenges metrics\n",
        "        challenge_scores = challenge_metrics(y_true, y_pred_binary, beta1=2, beta2=2)\n",
        "        results['F_beta_macro'] = challenge_scores['F_beta_macro']\n",
        "        results['G_beta_macro'] = challenge_scores['G_beta_macro']\n",
        "\n",
        "    # label based metric\n",
        "    results['macro_auc'] = roc_auc_score(y_true, y_pred, average='macro')\n",
        "    \n",
        "    df_result = pd.DataFrame(results, index=[0])\n",
        "    return df_result\n",
        "\n",
        "def challenge_metrics(y_true, y_pred, beta1=2, beta2=2, class_weights=None, single=False):\n",
        "    f_beta = 0\n",
        "    g_beta = 0\n",
        "    if single: # if evaluating single class in case of threshold-optimization\n",
        "        sample_weights = np.ones(y_true.sum(axis=1).shape)\n",
        "    else:\n",
        "        sample_weights = y_true.sum(axis=1)\n",
        "    for classi in range(y_true.shape[1]):\n",
        "        y_truei, y_predi = y_true[:,classi], y_pred[:,classi]\n",
        "        TP, FP, TN, FN = 0.,0.,0.,0.\n",
        "        for i in range(len(y_predi)):\n",
        "            sample_weight = sample_weights[i]\n",
        "            if y_truei[i]==y_predi[i]==1: \n",
        "                TP += 1./sample_weight\n",
        "            if ((y_predi[i]==1) and (y_truei[i]!=y_predi[i])): \n",
        "                FP += 1./sample_weight\n",
        "            if y_truei[i]==y_predi[i]==0: \n",
        "                TN += 1./sample_weight\n",
        "            if ((y_predi[i]==0) and (y_truei[i]!=y_predi[i])): \n",
        "                FN += 1./sample_weight \n",
        "        f_beta_i = ((1+beta1**2)*TP)/((1+beta1**2)*TP + FP + (beta1**2)*FN)\n",
        "        g_beta_i = (TP)/(TP+FP+beta2*FN)\n",
        "\n",
        "        f_beta += f_beta_i\n",
        "        g_beta += g_beta_i\n",
        "\n",
        "    return {'F_beta_macro':f_beta/y_true.shape[1], 'G_beta_macro':g_beta/y_true.shape[1]}\n",
        "\n",
        "def get_appropriate_bootstrap_samples(y_true, n_bootstraping_samples):\n",
        "    samples=[]\n",
        "    while True:\n",
        "        ridxs = np.random.randint(0, len(y_true), len(y_true))\n",
        "        if y_true[ridxs].sum(axis=0).min() != 0:\n",
        "            samples.append(ridxs)\n",
        "            if len(samples) == n_bootstraping_samples:\n",
        "                break\n",
        "    return samples\n",
        "\n",
        "def find_optimal_cutoff_threshold(target, predicted):\n",
        "    \"\"\" \n",
        "    Find the optimal probability cutoff point for a classification model related to event rate\n",
        "    \"\"\"\n",
        "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "def find_optimal_cutoff_thresholds(y_true, y_pred):\n",
        "\treturn [find_optimal_cutoff_threshold(y_true[:,i], y_pred[:,i]) for i in range(y_true.shape[1])]\n",
        "\n",
        "def find_optimal_cutoff_threshold_for_Gbeta(target, predicted, n_thresholds=100):\n",
        "    thresholds = np.linspace(0.00,1,n_thresholds)\n",
        "    scores = [challenge_metrics(target, predicted>t, single=True)['G_beta_macro'] for t in thresholds]\n",
        "    optimal_idx = np.argmax(scores)\n",
        "    return thresholds[optimal_idx]\n",
        "\n",
        "def find_optimal_cutoff_thresholds_for_Gbeta(y_true, y_pred):\n",
        "    print(\"optimize thresholds with respect to G_beta\")\n",
        "    return [find_optimal_cutoff_threshold_for_Gbeta(y_true[:,k][:,np.newaxis], y_pred[:,k][:,np.newaxis]) for k in tqdm(range(y_true.shape[1]))]\n",
        "\n",
        "def apply_thresholds(preds, thresholds):\n",
        "\t\"\"\"\n",
        "\t\tapply class-wise thresholds to prediction score in order to get binary format.\n",
        "\t\tBUT: if no score is above threshold, pick maximum. This is needed due to metric issues.\n",
        "\t\"\"\"\n",
        "\ttmp = []\n",
        "\tfor p in preds:\n",
        "\t\ttmp_p = (p > thresholds).astype(int)\n",
        "\t\tif np.sum(tmp_p) == 0:\n",
        "\t\t\ttmp_p[np.argmax(p)] = 1\n",
        "\t\ttmp.append(tmp_p)\n",
        "\ttmp = np.array(tmp)\n",
        "\treturn tmp\n",
        "\n",
        "# DATA PROCESSING STUFF\n",
        "\n",
        "def load_dataset(path, sampling_rate, release=False):\n",
        "    if path.split('/')[-2] == 'ptbxl':\n",
        "        # load and convert annotation data\n",
        "        Y = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
        "        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        # Load raw signal data\n",
        "        X = load_raw_data_ptbxl(Y, sampling_rate, path)\n",
        "\n",
        "    elif path.split('/')[-2] == 'ICBEB':\n",
        "        # load and convert annotation data\n",
        "        Y = pd.read_csv(path+'icbeb_database.csv', index_col='ecg_id')\n",
        "        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        # Load raw signal data\n",
        "        X = load_raw_data_icbeb(Y, sampling_rate, path)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def load_raw_data_icbeb(df, sampling_rate, path):\n",
        "\n",
        "    if sampling_rate == 100:\n",
        "        if os.path.exists(path + 'raw100.npy'):\n",
        "            data = np.load(path+'raw100.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path + 'records100/'+str(f)) for f in tqdm(df.index)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw100.npy', 'wb'), protocol=4)\n",
        "    elif sampling_rate == 500:\n",
        "        if os.path.exists(path + 'raw500.npy'):\n",
        "            data = np.load(path+'raw500.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path + 'records500/'+str(f)) for f in tqdm(df.index)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw500.npy', 'wb'), protocol=4)\n",
        "    return data\n",
        "\n",
        "def load_raw_data_ptbxl(df, sampling_rate, path):\n",
        "    if sampling_rate == 100:\n",
        "        if os.path.exists(path + 'raw100.npy'):\n",
        "            data = np.load(path+'raw100.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path+f) for f in tqdm(df.filename_lr)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw100.npy', 'wb'), protocol=4)\n",
        "    elif sampling_rate == 500:\n",
        "        if os.path.exists(path + 'raw500.npy'):\n",
        "            data = np.load(path+'raw500.npy', allow_pickle=True)\n",
        "        else:\n",
        "            data = [wfdb.rdsamp(path+f) for f in tqdm(df.filename_hr)]\n",
        "            data = np.array([signal for signal, meta in data])\n",
        "            pickle.dump(data, open(path+'raw500.npy', 'wb'), protocol=4)\n",
        "    return data\n",
        "\n",
        "def compute_label_aggregations(df, folder, ctype):\n",
        "\n",
        "    df['scp_codes_len'] = df.scp_codes.apply(lambda x: len(x))\n",
        "\n",
        "    aggregation_df = pd.read_csv(folder+'scp_statements.csv', index_col=0)\n",
        "\n",
        "    if ctype in ['diagnostic', 'subdiagnostic', 'superdiagnostic']:\n",
        "\n",
        "        def aggregate_all_diagnostic(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in diag_agg_df.index:\n",
        "                    tmp.append(key)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        def aggregate_subdiagnostic(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in diag_agg_df.index:\n",
        "                    c = diag_agg_df.loc[key].diagnostic_subclass\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        def aggregate_diagnostic(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in diag_agg_df.index:\n",
        "                    c = diag_agg_df.loc[key].diagnostic_class\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        diag_agg_df = aggregation_df[aggregation_df.diagnostic == 1.0]\n",
        "        if ctype == 'diagnostic':\n",
        "            df['diagnostic'] = df.scp_codes.apply(aggregate_all_diagnostic)\n",
        "            df['diagnostic_len'] = df.diagnostic.apply(lambda x: len(x))\n",
        "        elif ctype == 'subdiagnostic':\n",
        "            df['subdiagnostic'] = df.scp_codes.apply(aggregate_subdiagnostic)\n",
        "            df['subdiagnostic_len'] = df.subdiagnostic.apply(lambda x: len(x))\n",
        "        elif ctype == 'superdiagnostic':\n",
        "            df['superdiagnostic'] = df.scp_codes.apply(aggregate_diagnostic)\n",
        "            df['superdiagnostic_len'] = df.superdiagnostic.apply(lambda x: len(x))\n",
        "    elif ctype == 'form':\n",
        "        form_agg_df = aggregation_df[aggregation_df.form == 1.0]\n",
        "\n",
        "        def aggregate_form(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in form_agg_df.index:\n",
        "                    c = key\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        df['form'] = df.scp_codes.apply(aggregate_form)\n",
        "        df['form_len'] = df.form.apply(lambda x: len(x))\n",
        "    elif ctype == 'rhythm':\n",
        "        rhythm_agg_df = aggregation_df[aggregation_df.rhythm == 1.0]\n",
        "\n",
        "        def aggregate_rhythm(y_dic):\n",
        "            tmp = []\n",
        "            for key in y_dic.keys():\n",
        "                if key in rhythm_agg_df.index:\n",
        "                    c = key\n",
        "                    if str(c) != 'nan':\n",
        "                        tmp.append(c)\n",
        "            return list(set(tmp))\n",
        "\n",
        "        df['rhythm'] = df.scp_codes.apply(aggregate_rhythm)\n",
        "        df['rhythm_len'] = df.rhythm.apply(lambda x: len(x))\n",
        "    elif ctype == 'all':\n",
        "        df['all_scp'] = df.scp_codes.apply(lambda x: list(set(x.keys())))\n",
        "\n",
        "    return df\n",
        "\n",
        "def select_data(XX,YY, ctype, min_samples, outputfolder):\n",
        "    # convert multilabel to multi-hot\n",
        "    mlb = MultiLabelBinarizer()\n",
        "\n",
        "    if ctype == 'diagnostic':\n",
        "        X = XX[YY.diagnostic_len > 0]\n",
        "        Y = YY[YY.diagnostic_len > 0]\n",
        "        mlb.fit(Y.diagnostic.values)\n",
        "        y = mlb.transform(Y.diagnostic.values)\n",
        "    elif ctype == 'subdiagnostic':\n",
        "        counts = pd.Series(np.concatenate(YY.subdiagnostic.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.subdiagnostic = YY.subdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))\n",
        "        X = XX[YY.subdiagnostic_len > 0]\n",
        "        Y = YY[YY.subdiagnostic_len > 0]\n",
        "        mlb.fit(Y.subdiagnostic.values)\n",
        "        y = mlb.transform(Y.subdiagnostic.values)\n",
        "    elif ctype == 'superdiagnostic':\n",
        "        counts = pd.Series(np.concatenate(YY.superdiagnostic.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.superdiagnostic = YY.superdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['superdiagnostic_len'] = YY.superdiagnostic.apply(lambda x: len(x))\n",
        "        X = XX[YY.superdiagnostic_len > 0]\n",
        "        Y = YY[YY.superdiagnostic_len > 0]\n",
        "        mlb.fit(Y.superdiagnostic.values)\n",
        "        y = mlb.transform(Y.superdiagnostic.values)\n",
        "    elif ctype == 'form':\n",
        "        # filter\n",
        "        counts = pd.Series(np.concatenate(YY.form.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.form = YY.form.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['form_len'] = YY.form.apply(lambda x: len(x))\n",
        "        # select\n",
        "        X = XX[YY.form_len > 0]\n",
        "        Y = YY[YY.form_len > 0]\n",
        "        mlb.fit(Y.form.values)\n",
        "        y = mlb.transform(Y.form.values)\n",
        "    elif ctype == 'rhythm':\n",
        "        # filter \n",
        "        counts = pd.Series(np.concatenate(YY.rhythm.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.rhythm = YY.rhythm.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['rhythm_len'] = YY.rhythm.apply(lambda x: len(x))\n",
        "        # select\n",
        "        X = XX[YY.rhythm_len > 0]\n",
        "        Y = YY[YY.rhythm_len > 0]\n",
        "        mlb.fit(Y.rhythm.values)\n",
        "        y = mlb.transform(Y.rhythm.values)\n",
        "    elif ctype == 'all':\n",
        "        # filter \n",
        "        counts = pd.Series(np.concatenate(YY.all_scp.values)).value_counts()\n",
        "        counts = counts[counts > min_samples]\n",
        "        YY.all_scp = YY.all_scp.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
        "        YY['all_scp_len'] = YY.all_scp.apply(lambda x: len(x))\n",
        "        # select\n",
        "        X = XX[YY.all_scp_len > 0]\n",
        "        Y = YY[YY.all_scp_len > 0]\n",
        "        mlb.fit(Y.all_scp.values)\n",
        "        y = mlb.transform(Y.all_scp.values)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    # save LabelBinarizer\n",
        "    with open(outputfolder+'mlb.pkl', 'wb') as tokenizer:\n",
        "        pickle.dump(mlb, tokenizer)\n",
        "\n",
        "    return X, Y, y, mlb\n",
        "\n",
        "def preprocess_signals(X_train, X_validation, X_test, outputfolder):\n",
        "    # Standardize data such that mean 0 and variance 1\n",
        "    ss = StandardScaler()\n",
        "    ss.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n",
        "    \n",
        "    # Save Standardizer data\n",
        "    with open(outputfolder+'standard_scaler.pkl', 'wb') as ss_file:\n",
        "        pickle.dump(ss, ss_file)\n",
        "\n",
        "    return apply_standardizer(X_train, ss), apply_standardizer(X_validation, ss), apply_standardizer(X_test, ss)\n",
        "\n",
        "def apply_standardizer(X, ss):\n",
        "    X_tmp = []\n",
        "    for x in X:\n",
        "        x_shape = x.shape\n",
        "        X_tmp.append(ss.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n",
        "    X_tmp = np.array(X_tmp)\n",
        "    return X_tmp\n",
        "\n",
        "\n",
        "# DOCUMENTATION STUFF\n",
        "\n",
        "def generate_ptbxl_summary_table(selection=None, folder='../output/'):\n",
        "\n",
        "    exps = ['exp0', 'exp1', 'exp1.1', 'exp1.1.1', 'exp2', 'exp3']\n",
        "    metric1 = 'macro_auc' \n",
        "\n",
        "    # get models\n",
        "    models = {}\n",
        "    for i, exp in enumerate(exps):\n",
        "        if selection is None:\n",
        "            exp_models = [m.split('/')[-1] for m in glob.glob(folder+str(exp)+'/models/*')]\n",
        "        else:\n",
        "            exp_models = selection\n",
        "        if i == 0:\n",
        "            models = set(exp_models)\n",
        "        else:\n",
        "            models = models.union(set(exp_models))\n",
        "\n",
        "    results_dic = {'Method':[], \n",
        "                'exp0_AUC':[], \n",
        "                'exp1_AUC':[], \n",
        "                'exp1.1_AUC':[], \n",
        "                'exp1.1.1_AUC':[], \n",
        "                'exp2_AUC':[],\n",
        "                'exp3_AUC':[]\n",
        "                }\n",
        "\n",
        "    for m in models:\n",
        "        results_dic['Method'].append(m)\n",
        "        \n",
        "        for e in exps:\n",
        "            \n",
        "            try:\n",
        "                me_res = pd.read_csv(folder+str(e)+'/models/'+str(m)+'/results/te_results.csv', index_col=0)\n",
        "    \n",
        "                mean1 = me_res.loc['point'][metric1]\n",
        "                unc1 = max(me_res.loc['upper'][metric1]-me_res.loc['point'][metric1], me_res.loc['point'][metric1]-me_res.loc['lower'][metric1])\n",
        "\n",
        "                results_dic[e+'_AUC'].append(\"%.3f(%.2d)\" %(np.round(mean1,3), int(unc1*1000)))\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                results_dic[e+'_AUC'].append(\"--\")\n",
        "            \n",
        "            \n",
        "    df = pd.DataFrame(results_dic)\n",
        "    df_index = df[df.Method.isin(['naive', 'ensemble'])]\n",
        "    df_rest = df[~df.Method.isin(['naive', 'ensemble'])]\n",
        "    df = pd.concat([df_rest, df_index])\n",
        "    df.to_csv(folder+'results_ptbxl.csv')\n",
        "\n",
        "    titles = [\n",
        "        '### 1. PTB-XL: all statements',\n",
        "        '### 2. PTB-XL: diagnostic statements',\n",
        "        '### 3. PTB-XL: Diagnostic subclasses',\n",
        "        '### 4. PTB-XL: Diagnostic superclasses',\n",
        "        '### 5. PTB-XL: Form statements',\n",
        "        '### 6. PTB-XL: Rhythm statements'        \n",
        "    ]\n",
        "\n",
        "    # helper output function for markdown tables\n",
        "    our_work = 'https://arxiv.org/abs/2004.13701'\n",
        "    our_repo = 'https://github.com/helme/ecg_ptbxl_benchmarking/'\n",
        "    md_source = ''\n",
        "    for i, e in enumerate(exps):\n",
        "        md_source += '\\n '+titles[i]+' \\n \\n'\n",
        "        md_source += '| Model | AUC &darr; | paper/source | code | \\n'\n",
        "        md_source += '|---:|:---|:---|:---| \\n'\n",
        "        for row in df_rest[['Method', e+'_AUC']].sort_values(e+'_AUC', ascending=False).values:\n",
        "            md_source += '| ' + row[0].replace('fastai_', '') + ' | ' + row[1] + ' | [our work]('+our_work+') | [this repo]('+our_repo+')| \\n'\n",
        "    print(md_source)\n",
        "\n",
        "def ICBEBE_table(selection=None, folder='../output/'):\n",
        "    cols = ['macro_auc', 'F_beta_macro', 'G_beta_macro']\n",
        "\n",
        "    if selection is None:\n",
        "        models = [m.split('/')[-1].split('_pretrained')[0] for m in glob.glob(folder+'exp_ICBEB/models/*')]\n",
        "    else:\n",
        "        models = [] \n",
        "        for s in selection:\n",
        "            #if s != 'Wavelet+NN':\n",
        "                models.append(s)\n",
        "\n",
        "    data = []\n",
        "    for model in models:\n",
        "        me_res = pd.read_csv(folder+'exp_ICBEB/models/'+model+'/results/te_results.csv', index_col=0)\n",
        "        mcol=[]\n",
        "        for col in cols:\n",
        "            mean = me_res.ix['point'][col]\n",
        "            unc = max(me_res.ix['upper'][col]-me_res.ix['point'][col], me_res.ix['point'][col]-me_res.ix['lower'][col])\n",
        "            mcol.append(\"%.3f(%.2d)\" %(np.round(mean,3), int(unc*1000)))\n",
        "        data.append(mcol)\n",
        "    data = np.array(data)\n",
        "\n",
        "    df = pd.DataFrame(data, columns=cols, index=models)\n",
        "    df.to_csv(folder+'results_icbeb.csv')\n",
        "\n",
        "    df_rest = df[~df.index.isin(['naive', 'ensemble'])]\n",
        "    df_rest = df_rest.sort_values('macro_auc', ascending=False)\n",
        "    our_work = 'https://arxiv.org/abs/2004.13701'\n",
        "    our_repo = 'https://github.com/helme/ecg_ptbxl_benchmarking/'\n",
        "\n",
        "    md_source = '| Model | AUC &darr; |  F_beta=2 | G_beta=2 | paper/source | code | \\n'\n",
        "    md_source += '|---:|:---|:---|:---|:---|:---| \\n'\n",
        "    for i, row in enumerate(df_rest[cols].values):\n",
        "        md_source += '| ' + df_rest.index[i].replace('fastai_', '') + ' | ' + row[0] + ' | ' + row[1] + ' | ' + row[2] + ' | [our work]('+our_work+') | [this repo]('+our_repo+')| \\n'\n",
        "    print(md_source)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfvtfMA2K2sw"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import argparse\n",
        "import scipy.io as sio\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import scipy\n",
        "# from tensorflow.python.client import device_lib\n",
        "# import keras\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, GRU, TimeDistributed, Bidirectional, LeakyReLU\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten,  Input, Reshape, GRU#, CuDNNGRU\n",
        "# from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPool1D, GlobalAveragePooling1D,concatenate,AveragePooling1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "import scipy.io as sio\n",
        "from os import listdir\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, fbeta_score, roc_auc_score, roc_curve, roc_curve, auc , f1_score\n",
        "\n",
        "import math\n",
        "import mlflow\n",
        "import mlflow.tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDXOJhDHqCOz"
      },
      "source": [
        "#sampling frequency=100\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO4FB4Qbxrfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e7a090-a6cd-40fe-9104-8a744c7ff770"
      },
      "source": [
        "from utils import utils\n",
        "\n",
        "sampling_frequency=100\n",
        "datafolder= '/gdrive/My Drive/ICBEB/'\n",
        "task='all'\n",
        "outputfolder='../output0/'\n",
        "\n",
        "# Load data\n",
        "data, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
        "# Preprocess label data\n",
        "labels = utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
        "# Select relevant data and convert to one-hot\n",
        "data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
        "\n",
        "# 1-9 for training \n",
        "X_train = data[labels.strat_fold < 10]\n",
        "y_train = Y[labels.strat_fold < 10]\n",
        "# 10 for validation\n",
        "X_val = data[labels.strat_fold == 10]\n",
        "y_val = Y[labels.strat_fold == 10]\n",
        "\n",
        "num_classes = 9         # <=== number of classes in the finetuning dataset\n",
        "input_shape = [1000,12] # <=== shape of samples, [None, 12] in case of different lengths\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6187,), (6187, 9), (690,), (690, 9))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxfT8OKac8vC",
        "outputId": "aab80ce3-528e-4aa2-8c3b-db6117b0d449"
      },
      "source": [
        "X_train[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtV-zMEK1GOb",
        "outputId": "55b9e9a4-0b57-4283-8022-8994309555e8"
      },
      "source": [
        "X_tr = []\n",
        "for i in range(len(X_train)):\n",
        "    x = []\n",
        "    for j in range(12):\n",
        "        p = X_train[i][:1000,j]\n",
        "        if p.shape[0]!= 1000:\n",
        "            d = abs(p.shape[0]-1000)//2\n",
        "            p = np.pad(p,(d,d))\n",
        "        x.append(p)\n",
        "    X_tr.append(np.transpose(x))\n",
        "X_tr = np.array(X_tr)\n",
        "X_tr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6187, 1000, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qpz5wPKJOxS",
        "outputId": "fb417035-471a-4dd6-f7ca-43dd6388a40b"
      },
      "source": [
        "X_te = []\n",
        "for i in range(len(X_val)):\n",
        "    x = []\n",
        "    for j in range(12):\n",
        "        p = X_val[i][:1000,j]\n",
        "        if p.shape[0]!= 1000:\n",
        "            d = abs(p.shape[0]-1000)//2\n",
        "            p = np.pad(p,(d,d))\n",
        "        x.append(p)\n",
        "    X_te.append(np.transpose(x))\n",
        "X_te = np.array(X_te)\n",
        "X_te.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(690, 1000, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFdluV6Ic2zE",
        "outputId": "420ca56c-ac42-4d25-a36e-9e9855472269"
      },
      "source": [
        "cd /gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_C8dQjNcCow"
      },
      "source": [
        "def dot_product(x, kernel):\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs): \n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform') \n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer) \n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint) \n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        " \n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint) \n",
        "            self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint) \n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        " \n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        " \n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W) \n",
        "        if self.bias:\n",
        "            uit += self.b \n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u) \n",
        "        a = K.exp(ait)\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) \n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        " \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz2Up_g3xNNO"
      },
      "source": [
        "main_input = Input(shape=(1000,12), dtype='float32', name='main_input')\n",
        "x = Convolution1D(12, 3, padding='same')(main_input)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(12, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Convolution1D(12, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(12, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Convolution1D(12, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(12, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Convolution1D(24, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(24, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(24, 24, strides = 2, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Convolution1D(24, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(24, 3, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Convolution1D(24, 48, strides = 2, padding='same')(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "cnnout = Dropout(0.2)(x)\n",
        "x = Bidirectional(GRU(24, input_shape=(32,24),return_sequences=True,return_state=False))(cnnout)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = AttentionWithContext()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "main_output = Dense(num_classes,activation='sigmoid')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dadm3EOPdeK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5abb18f-e5b0-4186-904f-3a785c3a23ac"
      },
      "source": [
        "model = Model(main_input,main_output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "main_input (InputLayer)      [(None, 1000, 12)]        0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 1000, 12)          444       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 1000, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1000, 12)          444       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 1000, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 500, 12)           3468      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 500, 12)           0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500, 12)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 500, 12)           444       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 500, 12)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 500, 12)           444       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 500, 12)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 250, 12)           3468      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 250, 12)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 250, 12)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 250, 12)           444       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 250, 12)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 250, 12)           444       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 250, 12)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 125, 12)           3468      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 125, 12)           0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 125, 12)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 125, 24)           888       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 125, 24)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 125, 24)           1752      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 125, 24)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_11 (Conv1D)           (None, 63, 24)            13848     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 63, 24)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 63, 24)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 63, 24)            1752      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 63, 24)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 63, 24)            1752      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 63, 24)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 32, 24)            27672     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 32, 24)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32, 24)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 32, 48)            7200      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 32, 48)            0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32, 48)            0         \n",
            "_________________________________________________________________\n",
            "attention_with_context (Atte (None, 48)                2400      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 48)                192       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 48)                0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 48)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 9)                 441       \n",
            "=================================================================\n",
            "Total params: 70,965\n",
            "Trainable params: 70,869\n",
            "Non-trainable params: 96\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm8jG2wQQ_72"
      },
      "source": [
        "mlflow.tensorflow.autolog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6VM5PffRLTx",
        "outputId": "270e9a71-dc87-40b0-908b-3ba0d600a85a"
      },
      "source": [
        "with mlflow.start_run() as run:\n",
        "\n",
        "    def find_optimal_cutoff_threshold(target, predicted):\n",
        "        \"\"\" \n",
        "        Find the optimal probability cutoff point for a classification model related to event rate\n",
        "        \"\"\"\n",
        "        fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "        optimal_idx = np.argmax(tpr - fpr)\n",
        "        optimal_threshold = threshold[optimal_idx]\n",
        "        return optimal_threshold\n",
        "\n",
        "    def find_optimal_cutoff_thresholds(y_true, y_pred):\n",
        "\t    return [find_optimal_cutoff_threshold(y_true[:,i], y_pred[:,i]) for i in range(y_true.shape[1])]\n",
        "\n",
        "    def apply_thresholds(preds, thresholds):\n",
        "        \"\"\"\n",
        "            apply class-wise thresholds to prediction score in order to get binary format.\n",
        "            BUT: if no score is above threshold, pick maximum. This is needed due to metric issues.\n",
        "        \"\"\"\n",
        "        tmp = []\n",
        "        for p in preds:\n",
        "            tmp_p = (p > thresholds).astype(int)\n",
        "            if np.sum(tmp_p) == 0:\n",
        "                tmp_p[np.argmax(p)] = 1\n",
        "            tmp.append(tmp_p)\n",
        "        tmp = np.array(tmp)\n",
        "        return tmp\n",
        "\n",
        "    def step_decay(epoch):\n",
        "        initial_lrate = 0.001\n",
        "        drop = 0.4\n",
        "        epochs_drop = 20.0\n",
        "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "        return lrate\n",
        "\n",
        "    lscheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "\n",
        "    adam = tf.keras.optimizers.Adam(0.001)\n",
        "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['AUC'])\n",
        "\n",
        "    num_folds = 8\n",
        "    epochs = 25\n",
        "    batch_size= 16\n",
        "    kfold = KFold(num_folds)\n",
        "\n",
        "    histories = []\n",
        "    fold_no = 1\n",
        "    for train, test in kfold.split(X_tr, y_train):\n",
        "\n",
        "        h = model.fit(X_tr[train], y_train[train],\n",
        "                        epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=(X_tr[test],y_train[test]),\n",
        "                        callbacks=[lscheduler])\n",
        "\n",
        "        print(\"iteration \", str(fold_no))\n",
        "        histories.append(h)\n",
        "        fold_no += 1\n",
        "\n",
        "    # predictions = model.predict(X_te)\n",
        "    test_auc = model.evaluate(X_te, y_val)[1]\n",
        "    print(\"Test auc avg: \", test_auc)\n",
        "\n",
        "    y_pred = model.predict(X_te)\n",
        "    thresholds =  find_optimal_cutoff_thresholds_for_Gbeta(y_val , y_pred)\n",
        "    y_pred_binary = apply_thresholds(y_pred, thresholds)\n",
        "\n",
        "    f1 = f1_score(y_val, y_pred_binary, average='macro')\n",
        "\n",
        "    mlflow.log_metric(\"Test AUC\", test_auc) \n",
        "    mlflow.log_metric('macro_f1_score' , f1) \n",
        "\n",
        "    mlflow.keras.log_model(model, \"my_model\")\n",
        "                        #    custom_objects=\n",
        "                        #    {\"log_cosh_dice_loss\": log_cosh_dice_loss,\"dice_coef\":dice_coef})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/339 [..............................] - ETA: 2:56 - loss: 0.6925 - auc: 0.5527WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_begin` time: 0.0987s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_end` time: 0.0707s). Check your callbacks.\n",
            "339/339 [==============================] - 44s 21ms/step - loss: 0.4162 - auc: 0.6616 - val_loss: 0.2932 - val_auc: 0.7758\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 4s 13ms/step - loss: 0.2968 - auc: 0.7672 - val_loss: 0.2786 - val_auc: 0.7989\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.2742 - auc: 0.8162 - val_loss: 0.3317 - val_auc: 0.8216\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 4s 13ms/step - loss: 0.2401 - auc: 0.8724 - val_loss: 0.2575 - val_auc: 0.8821\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.2129 - auc: 0.9039 - val_loss: 0.1993 - val_auc: 0.9209\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1986 - auc: 0.9159 - val_loss: 0.1853 - val_auc: 0.9283\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1884 - auc: 0.9255 - val_loss: 0.1700 - val_auc: 0.9414\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1777 - auc: 0.9335 - val_loss: 0.1690 - val_auc: 0.9452\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1764 - auc: 0.9350 - val_loss: 0.1599 - val_auc: 0.9489\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1703 - auc: 0.9381 - val_loss: 0.1661 - val_auc: 0.9426\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1669 - auc: 0.9428 - val_loss: 0.1544 - val_auc: 0.9514\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1619 - auc: 0.9453 - val_loss: 0.1653 - val_auc: 0.9446\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1570 - auc: 0.9481 - val_loss: 0.1719 - val_auc: 0.9469\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1559 - auc: 0.9491 - val_loss: 0.1540 - val_auc: 0.9530\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1491 - auc: 0.9527 - val_loss: 0.1453 - val_auc: 0.9555\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1485 - auc: 0.9525 - val_loss: 0.1392 - val_auc: 0.9603\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1446 - auc: 0.9558 - val_loss: 0.1525 - val_auc: 0.9525\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1472 - auc: 0.9548 - val_loss: 0.1861 - val_auc: 0.9406\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1418 - auc: 0.9577 - val_loss: 0.1448 - val_auc: 0.9574\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1288 - auc: 0.9642 - val_loss: 0.1262 - val_auc: 0.9671\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1240 - auc: 0.9672 - val_loss: 0.1331 - val_auc: 0.9633\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1225 - auc: 0.9677 - val_loss: 0.1349 - val_auc: 0.9639\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1218 - auc: 0.9677 - val_loss: 0.1353 - val_auc: 0.9652\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1197 - auc: 0.9686 - val_loss: 0.1356 - val_auc: 0.9599\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1190 - auc: 0.9695 - val_loss: 0.1349 - val_auc: 0.9637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpjlwmi8es/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpjlwmi8es/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  1\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021/08/10 04:31:26 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='70506b0813344660b21a821eea68a11d'. Attempted logging new value '0.0004'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/339 [..............................] - ETA: 2:15 - loss: 0.1140 - auc: 0.9716WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_begin` time: 0.0490s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_begin` time: 0.0490s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_end` time: 0.0809s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_end` time: 0.0809s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "339/339 [==============================] - 5s 16ms/step - loss: 0.1348 - auc: 0.9608 - val_loss: 0.1340 - val_auc: 0.9655\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1327 - auc: 0.9610 - val_loss: 0.1393 - val_auc: 0.9629\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1301 - auc: 0.9634 - val_loss: 0.1479 - val_auc: 0.9573\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1320 - auc: 0.9624 - val_loss: 0.1186 - val_auc: 0.9731\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1273 - auc: 0.9644 - val_loss: 0.1395 - val_auc: 0.9611\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1257 - auc: 0.9654 - val_loss: 0.1575 - val_auc: 0.9616\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1229 - auc: 0.9672 - val_loss: 0.1410 - val_auc: 0.9630\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1243 - auc: 0.9666 - val_loss: 0.1441 - val_auc: 0.9614\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1254 - auc: 0.9658 - val_loss: 0.1528 - val_auc: 0.9553\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1224 - auc: 0.9670 - val_loss: 0.1546 - val_auc: 0.9597\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1232 - auc: 0.9657 - val_loss: 0.1335 - val_auc: 0.9678\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1198 - auc: 0.9688 - val_loss: 0.1269 - val_auc: 0.9659\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1213 - auc: 0.9679 - val_loss: 0.1425 - val_auc: 0.9607\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1207 - auc: 0.9677 - val_loss: 0.1348 - val_auc: 0.9673\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1183 - auc: 0.9695 - val_loss: 0.1529 - val_auc: 0.9568\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1185 - auc: 0.9693 - val_loss: 0.1372 - val_auc: 0.9626\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1171 - auc: 0.9702 - val_loss: 0.1414 - val_auc: 0.9562\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1179 - auc: 0.9688 - val_loss: 0.1387 - val_auc: 0.9631\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1164 - auc: 0.9701 - val_loss: 0.1492 - val_auc: 0.9617\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1033 - auc: 0.9765 - val_loss: 0.1333 - val_auc: 0.9654\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1021 - auc: 0.9768 - val_loss: 0.1397 - val_auc: 0.9620\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0979 - auc: 0.9787 - val_loss: 0.1459 - val_auc: 0.9634\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0991 - auc: 0.9777 - val_loss: 0.1362 - val_auc: 0.9672\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0975 - auc: 0.9796 - val_loss: 0.1406 - val_auc: 0.9642\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0952 - auc: 0.9797 - val_loss: 0.1379 - val_auc: 0.9632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpfr2coezx/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpfr2coezx/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  2\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021/08/10 04:33:45 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='70506b0813344660b21a821eea68a11d'. Attempted logging new value '0.0004'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/339 [..............................] - ETA: 1:58 - loss: 0.1057 - auc: 0.9739WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_begin` time: 0.0469s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_begin` time: 0.0469s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0656s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0656s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "339/339 [==============================] - 5s 16ms/step - loss: 0.1175 - auc: 0.9696 - val_loss: 0.0988 - val_auc: 0.9815\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1193 - auc: 0.9692 - val_loss: 0.1077 - val_auc: 0.9786\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1152 - auc: 0.9712 - val_loss: 0.0942 - val_auc: 0.9817\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1127 - auc: 0.9728 - val_loss: 0.1069 - val_auc: 0.9795\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1153 - auc: 0.9712 - val_loss: 0.1240 - val_auc: 0.9775\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1136 - auc: 0.9716 - val_loss: 0.1007 - val_auc: 0.9811\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1139 - auc: 0.9719 - val_loss: 0.1146 - val_auc: 0.9755\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1115 - auc: 0.9721 - val_loss: 0.1053 - val_auc: 0.9805\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1092 - auc: 0.9756 - val_loss: 0.1246 - val_auc: 0.9732\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1124 - auc: 0.9720 - val_loss: 0.1112 - val_auc: 0.9782\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1106 - auc: 0.9732 - val_loss: 0.1087 - val_auc: 0.9798\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1088 - auc: 0.9747 - val_loss: 0.0993 - val_auc: 0.9802\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1068 - auc: 0.9757 - val_loss: 0.1024 - val_auc: 0.9806\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1126 - auc: 0.9725 - val_loss: 0.1002 - val_auc: 0.9799\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1129 - auc: 0.9720 - val_loss: 0.1037 - val_auc: 0.9791\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1113 - auc: 0.9729 - val_loss: 0.1079 - val_auc: 0.9782\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1086 - auc: 0.9738 - val_loss: 0.1071 - val_auc: 0.9769\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1053 - auc: 0.9771 - val_loss: 0.1048 - val_auc: 0.9800\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1090 - auc: 0.9740 - val_loss: 0.1339 - val_auc: 0.9707\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0936 - auc: 0.9814 - val_loss: 0.1159 - val_auc: 0.9785\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0916 - auc: 0.9815 - val_loss: 0.1078 - val_auc: 0.9805\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0882 - auc: 0.9827 - val_loss: 0.1116 - val_auc: 0.9797\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0871 - auc: 0.9835 - val_loss: 0.1053 - val_auc: 0.9799\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0871 - auc: 0.9826 - val_loss: 0.1141 - val_auc: 0.9785\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0868 - auc: 0.9830 - val_loss: 0.1050 - val_auc: 0.9795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp6f_4b7wr/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp6f_4b7wr/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  3\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021/08/10 04:36:08 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='70506b0813344660b21a821eea68a11d'. Attempted logging new value '0.0004'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/339 [..............................] - ETA: 2:03 - loss: 0.0695 - auc: 0.9936WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_begin` time: 0.0506s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_begin` time: 0.0506s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0670s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0670s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "339/339 [==============================] - 6s 17ms/step - loss: 0.1095 - auc: 0.9739 - val_loss: 0.0820 - val_auc: 0.9870\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1060 - auc: 0.9753 - val_loss: 0.0893 - val_auc: 0.9840\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1080 - auc: 0.9750 - val_loss: 0.0837 - val_auc: 0.9862\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1046 - auc: 0.9761 - val_loss: 0.0994 - val_auc: 0.9778\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1047 - auc: 0.9760 - val_loss: 0.1009 - val_auc: 0.9784\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1034 - auc: 0.9771 - val_loss: 0.0879 - val_auc: 0.9850\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1053 - auc: 0.9763 - val_loss: 0.0986 - val_auc: 0.9802\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1036 - auc: 0.9773 - val_loss: 0.0946 - val_auc: 0.9804\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1046 - auc: 0.9765 - val_loss: 0.1035 - val_auc: 0.9753\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1039 - auc: 0.9765 - val_loss: 0.0980 - val_auc: 0.9785\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1013 - auc: 0.9782 - val_loss: 0.1060 - val_auc: 0.9750\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1030 - auc: 0.9777 - val_loss: 0.1043 - val_auc: 0.9764\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1020 - auc: 0.9771 - val_loss: 0.1045 - val_auc: 0.9757\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1021 - auc: 0.9775 - val_loss: 0.1174 - val_auc: 0.9717\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1009 - auc: 0.9781 - val_loss: 0.1096 - val_auc: 0.9738\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1013 - auc: 0.9782 - val_loss: 0.1209 - val_auc: 0.9663\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0993 - auc: 0.9785 - val_loss: 0.1044 - val_auc: 0.9752\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1005 - auc: 0.9778 - val_loss: 0.1143 - val_auc: 0.9731\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1021 - auc: 0.9774 - val_loss: 0.1153 - val_auc: 0.9706\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0888 - auc: 0.9834 - val_loss: 0.1016 - val_auc: 0.9772\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0842 - auc: 0.9840 - val_loss: 0.1112 - val_auc: 0.9753\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0826 - auc: 0.9847 - val_loss: 0.1109 - val_auc: 0.9773\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0801 - auc: 0.9856 - val_loss: 0.1046 - val_auc: 0.9787\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0776 - auc: 0.9864 - val_loss: 0.1037 - val_auc: 0.9764\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0779 - auc: 0.9864 - val_loss: 0.1091 - val_auc: 0.9770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpn9e1431n/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpn9e1431n/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  4\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021/08/10 04:39:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='70506b0813344660b21a821eea68a11d'. Attempted logging new value '0.0004'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/339 [..............................] - ETA: 2:06 - loss: 0.0990 - auc: 0.9825WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_begin` time: 0.0476s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_begin` time: 0.0476s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0724s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0724s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "339/339 [==============================] - 5s 16ms/step - loss: 0.0992 - auc: 0.9785 - val_loss: 0.0678 - val_auc: 0.9897\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1018 - auc: 0.9772 - val_loss: 0.0724 - val_auc: 0.9858\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0997 - auc: 0.9783 - val_loss: 0.0649 - val_auc: 0.9909\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1023 - auc: 0.9768 - val_loss: 0.0791 - val_auc: 0.9857\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0982 - auc: 0.9792 - val_loss: 0.0751 - val_auc: 0.9840\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0993 - auc: 0.9797 - val_loss: 0.0715 - val_auc: 0.9874\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0978 - auc: 0.9798 - val_loss: 0.0806 - val_auc: 0.9846\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0974 - auc: 0.9797 - val_loss: 0.0944 - val_auc: 0.9808\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0995 - auc: 0.9786 - val_loss: 0.0831 - val_auc: 0.9844\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0987 - auc: 0.9795 - val_loss: 0.0916 - val_auc: 0.9809\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1058 - auc: 0.9762 - val_loss: 0.0877 - val_auc: 0.9842\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0971 - auc: 0.9807 - val_loss: 0.0858 - val_auc: 0.9806\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0975 - auc: 0.9786 - val_loss: 0.1171 - val_auc: 0.9730\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0965 - auc: 0.9811 - val_loss: 0.0933 - val_auc: 0.9822\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0957 - auc: 0.9799 - val_loss: 0.0802 - val_auc: 0.9842\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0959 - auc: 0.9801 - val_loss: 0.1035 - val_auc: 0.9790\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0992 - auc: 0.9791 - val_loss: 0.0916 - val_auc: 0.9812\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0963 - auc: 0.9797 - val_loss: 0.0888 - val_auc: 0.9826\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0942 - auc: 0.9804 - val_loss: 0.1087 - val_auc: 0.9748\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0837 - auc: 0.9847 - val_loss: 0.0847 - val_auc: 0.9844\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0807 - auc: 0.9852 - val_loss: 0.0837 - val_auc: 0.9841\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0817 - auc: 0.9858 - val_loss: 0.0779 - val_auc: 0.9864\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0761 - auc: 0.9872 - val_loss: 0.0763 - val_auc: 0.9870\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0759 - auc: 0.9871 - val_loss: 0.0856 - val_auc: 0.9841\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0791 - auc: 0.9859 - val_loss: 0.0812 - val_auc: 0.9837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpniq6bi0c/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpniq6bi0c/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  5\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021/08/10 04:41:57 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='70506b0813344660b21a821eea68a11d'. Attempted logging new value '0.0004'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/339 [..............................] - ETA: 2:13 - loss: 0.0839 - auc: 0.9763WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_begin` time: 0.0489s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_begin` time: 0.0489s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.0780s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.0780s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "339/339 [==============================] - 6s 17ms/step - loss: 0.0980 - auc: 0.9789 - val_loss: 0.0803 - val_auc: 0.9875\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0976 - auc: 0.9786 - val_loss: 0.0844 - val_auc: 0.9862\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0944 - auc: 0.9809 - val_loss: 0.0887 - val_auc: 0.9852\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0941 - auc: 0.9802 - val_loss: 0.0905 - val_auc: 0.9848\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0933 - auc: 0.9812 - val_loss: 0.0826 - val_auc: 0.9871\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0927 - auc: 0.9801 - val_loss: 0.0956 - val_auc: 0.9842\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0929 - auc: 0.9807 - val_loss: 0.1042 - val_auc: 0.9817\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0919 - auc: 0.9824 - val_loss: 0.1018 - val_auc: 0.9822\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0944 - auc: 0.9804 - val_loss: 0.1164 - val_auc: 0.9747\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0962 - auc: 0.9800 - val_loss: 0.0982 - val_auc: 0.9817\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0950 - auc: 0.9804 - val_loss: 0.1192 - val_auc: 0.9734\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0970 - auc: 0.9789 - val_loss: 0.1041 - val_auc: 0.9776\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0917 - auc: 0.9821 - val_loss: 0.0986 - val_auc: 0.9810\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0927 - auc: 0.9811 - val_loss: 0.1047 - val_auc: 0.9824\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0909 - auc: 0.9816 - val_loss: 0.1057 - val_auc: 0.9798\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0944 - auc: 0.9801 - val_loss: 0.1073 - val_auc: 0.9764\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0940 - auc: 0.9806 - val_loss: 0.1125 - val_auc: 0.9778\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0960 - auc: 0.9788 - val_loss: 0.1190 - val_auc: 0.9728\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0939 - auc: 0.9807 - val_loss: 0.1058 - val_auc: 0.9775\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0816 - auc: 0.9852 - val_loss: 0.1035 - val_auc: 0.9796\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0751 - auc: 0.9875 - val_loss: 0.0979 - val_auc: 0.9811\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0728 - auc: 0.9877 - val_loss: 0.1004 - val_auc: 0.9798\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0734 - auc: 0.9882 - val_loss: 0.1030 - val_auc: 0.9796\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0720 - auc: 0.9886 - val_loss: 0.1087 - val_auc: 0.9767\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0741 - auc: 0.9877 - val_loss: 0.1101 - val_auc: 0.9773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8q8uu6ve/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp8q8uu6ve/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  6\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021/08/10 04:44:19 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='70506b0813344660b21a821eea68a11d'. Attempted logging new value '0.0004'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  6/339 [..............................] - ETA: 57s - loss: 0.0652 - auc: 0.9893 WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_begin` time: 0.0492s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_begin` time: 0.0492s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_end` time: 0.0797s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_end` time: 0.0797s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "339/339 [==============================] - 6s 17ms/step - loss: 0.1012 - auc: 0.9781 - val_loss: 0.0605 - val_auc: 0.9922\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0979 - auc: 0.9786 - val_loss: 0.0730 - val_auc: 0.9867\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0940 - auc: 0.9796 - val_loss: 0.0628 - val_auc: 0.9916\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0934 - auc: 0.9803 - val_loss: 0.0622 - val_auc: 0.9905\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0986 - auc: 0.9788 - val_loss: 0.0684 - val_auc: 0.9888\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0934 - auc: 0.9802 - val_loss: 0.0691 - val_auc: 0.9885\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0973 - auc: 0.9784 - val_loss: 0.0659 - val_auc: 0.9888\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0970 - auc: 0.9793 - val_loss: 0.0829 - val_auc: 0.9851\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0957 - auc: 0.9800 - val_loss: 0.0764 - val_auc: 0.9863\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0977 - auc: 0.9791 - val_loss: 0.0802 - val_auc: 0.9842\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0981 - auc: 0.9790 - val_loss: 0.0836 - val_auc: 0.9842\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0961 - auc: 0.9799 - val_loss: 0.0822 - val_auc: 0.9841\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0980 - auc: 0.9785 - val_loss: 0.0981 - val_auc: 0.9778\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0941 - auc: 0.9804 - val_loss: 0.0844 - val_auc: 0.9833\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0975 - auc: 0.9793 - val_loss: 0.0785 - val_auc: 0.9857\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0992 - auc: 0.9790 - val_loss: 0.0952 - val_auc: 0.9769\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1004 - auc: 0.9776 - val_loss: 0.0869 - val_auc: 0.9828\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0992 - auc: 0.9782 - val_loss: 0.0968 - val_auc: 0.9779\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0987 - auc: 0.9786 - val_loss: 0.0973 - val_auc: 0.9794\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0885 - auc: 0.9830 - val_loss: 0.0766 - val_auc: 0.9851\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0822 - auc: 0.9848 - val_loss: 0.0768 - val_auc: 0.9856\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0794 - auc: 0.9860 - val_loss: 0.0757 - val_auc: 0.9859\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0793 - auc: 0.9860 - val_loss: 0.0747 - val_auc: 0.9853\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0766 - auc: 0.9863 - val_loss: 0.0753 - val_auc: 0.9850\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.0778 - auc: 0.9863 - val_loss: 0.0739 - val_auc: 0.9845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpbmr9yg3x/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpbmr9yg3x/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  7\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionWithContext has arguments in `__init__` and therefore must override `get_config`.\n",
            "2021/08/10 04:46:44 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/tensorflow.py:781: UserWarning: Logging to MLflow failed: Changing param values is not allowed. Param with key='opt_learning_rate' was already logged with value='0.001' for run ID='70506b0813344660b21a821eea68a11d'. Attempted logging new value '0.0004'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  3/339 [..............................] - ETA: 2:06 - loss: 0.0807 - auc: 0.9808WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_begin` time: 0.0489s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_begin` time: 0.0489s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_end` time: 0.0712s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_end` time: 0.0712s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "339/339 [==============================] - 6s 17ms/step - loss: 0.0998 - auc: 0.9781 - val_loss: 0.0728 - val_auc: 0.9893\n",
            "Epoch 2/25\n",
            "339/339 [==============================] - 5s 16ms/step - loss: 0.1002 - auc: 0.9764 - val_loss: 0.0826 - val_auc: 0.9851\n",
            "Epoch 3/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0979 - auc: 0.9782 - val_loss: 0.0974 - val_auc: 0.9795\n",
            "Epoch 4/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1004 - auc: 0.9778 - val_loss: 0.0791 - val_auc: 0.9871\n",
            "Epoch 5/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0984 - auc: 0.9784 - val_loss: 0.0801 - val_auc: 0.9856\n",
            "Epoch 6/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1030 - auc: 0.9774 - val_loss: 0.0836 - val_auc: 0.9861\n",
            "Epoch 7/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1034 - auc: 0.9758 - val_loss: 0.0946 - val_auc: 0.9821\n",
            "Epoch 8/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.1001 - auc: 0.9782 - val_loss: 0.0904 - val_auc: 0.9838\n",
            "Epoch 9/25\n",
            "339/339 [==============================] - 5s 13ms/step - loss: 0.0996 - auc: 0.9773 - val_loss: 0.1063 - val_auc: 0.9776\n",
            "Epoch 10/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1035 - auc: 0.9761 - val_loss: 0.0972 - val_auc: 0.9826\n",
            "Epoch 11/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1036 - auc: 0.9754 - val_loss: 0.0917 - val_auc: 0.9821\n",
            "Epoch 12/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1112 - auc: 0.9718 - val_loss: 0.1026 - val_auc: 0.9761\n",
            "Epoch 13/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1066 - auc: 0.9745 - val_loss: 0.0997 - val_auc: 0.9775\n",
            "Epoch 14/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1090 - auc: 0.9735 - val_loss: 0.1227 - val_auc: 0.9687\n",
            "Epoch 15/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1065 - auc: 0.9740 - val_loss: 0.1043 - val_auc: 0.9760\n",
            "Epoch 16/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1222 - auc: 0.9683 - val_loss: 0.1088 - val_auc: 0.9776\n",
            "Epoch 17/25\n",
            "339/339 [==============================] - 5s 15ms/step - loss: 0.1130 - auc: 0.9716 - val_loss: 0.1194 - val_auc: 0.9733\n",
            "Epoch 18/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1207 - auc: 0.9678 - val_loss: 0.1225 - val_auc: 0.9712\n",
            "Epoch 19/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1165 - auc: 0.9712 - val_loss: 0.1291 - val_auc: 0.9662\n",
            "Epoch 20/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1113 - auc: 0.9723 - val_loss: 0.1103 - val_auc: 0.9764\n",
            "Epoch 21/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.1007 - auc: 0.9771 - val_loss: 0.1099 - val_auc: 0.9751\n",
            "Epoch 22/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0972 - auc: 0.9783 - val_loss: 0.1106 - val_auc: 0.9762\n",
            "Epoch 23/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0979 - auc: 0.9783 - val_loss: 0.1029 - val_auc: 0.9765\n",
            "Epoch 24/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0947 - auc: 0.9789 - val_loss: 0.0903 - val_auc: 0.9807\n",
            "Epoch 25/25\n",
            "339/339 [==============================] - 5s 14ms/step - loss: 0.0952 - auc: 0.9785 - val_loss: 0.1033 - val_auc: 0.9788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpd_d00781/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpd_d00781/model/data/model/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration  8\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.1448 - auc: 0.9622\n",
            "Test auc avg:  0.9622018933296204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/9 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "optimize thresholds with respect to G_beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:07<00:00,  1.13it/s]\n",
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp97ppgsxj/model/data/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "INFO:tensorflow:Assets written to: /tmp/tmp97ppgsxj/model/data/model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFzK4oq94GlW",
        "outputId": "a047c911-0f3e-4923-a4dd-a54b68dfd609"
      },
      "source": [
        " model.evaluate(X_te, y_val)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1448 - auc: 0.9622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9622018933296204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzXjfvp6zo-4",
        "outputId": "121ba8e6-f1ba-4f64-d341-91fcfd3100e1"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.0.6.tar.gz (746 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 35.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 204 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 215 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 225 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 409 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 430 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 440 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 450 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 460 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 471 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 614 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 624 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 634 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 645 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 655 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 665 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 686 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 696 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 706 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 746 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (5.4.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.0.6-py3-none-any.whl size=19263 sha256=c3e1099d19ff8cce421072013dd96f6fc3ac3c4f91894535a65de657fc2599d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/8c/c4/8d9cbca4fa19bf64887b4a91914194bb9033f1a7cbb344d5ab\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwU2yLuGGjRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d44667-3d1e-400d-9ca8-75eccd52117e"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "# Setting the authtoken (optional)\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"1w9tvYSl88onvrzvZJMxMcDDb9Y_3NHGDZssJWJ6rdcw9TsBN\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLflow Tracking UI: https://e7512564ee0c.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU0wpZe3Oa1L",
        "outputId": "96b0a27b-eb1a-4145-80aa-9821a46aa4d1"
      },
      "source": [
        "import mlflow\n",
        "logged_model = 'runs:/8647c6a76da74d45b199987c3afe9cc3/my_model'\n",
        "\n",
        "# Load model as a PyFuncModel.\n",
        "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
        "\n",
        "# Predict on a Pandas DataFrame.\n",
        "import pandas as pd\n",
        "loaded_model.predict(X_te)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:400: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0846902e-03, 9.9526972e-01, 3.6218837e-05, ..., 2.6635791e-03,\n",
              "        1.7385917e-04, 1.3035627e-03],\n",
              "       [4.2980540e-04, 6.9333529e-01, 8.7458706e-05, ..., 1.3823813e-01,\n",
              "        2.2161040e-03, 6.7096609e-03],\n",
              "       [3.9817058e-04, 2.1605857e-04, 4.0957024e-05, ..., 3.4943956e-03,\n",
              "        6.9021443e-03, 2.6161365e-02],\n",
              "       ...,\n",
              "       [4.2314068e-03, 4.1142190e-04, 3.9862874e-03, ..., 4.0415773e-04,\n",
              "        4.4818908e-02, 2.7394015e-03],\n",
              "       [3.4750886e-03, 1.3193399e-04, 1.3955474e-04, ..., 6.7647542e-03,\n",
              "        1.5713684e-01, 9.8844049e-03],\n",
              "       [2.7932329e-04, 3.1341080e-04, 1.4614439e-05, ..., 8.7186193e-01,\n",
              "        1.8799753e-04, 1.9078143e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geSk8GzJO4AZ",
        "outputId": "d7ec0150-86da-4b04-adae-7bc73fb1c1fa"
      },
      "source": [
        "preds = loaded_model.predict(X_te)\n",
        "preds.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(690, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcK6FR-EPRX_"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp3IrPi7fT0P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}